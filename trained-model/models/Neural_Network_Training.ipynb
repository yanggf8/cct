{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# Neural Network Training for Financial Predictions\n",
        "\n",
        "**Goal**: Train TFT and N-HITS models in Colab, export trained weights for Vercel deployment\n",
        "\n",
        "**Process**:\n",
        "1. Train models on real financial data in Colab (with GPU)\n",
        "2. Save trained weights in TensorFlow.js format\n",
        "3. Download weights to deploy on Vercel\n",
        "4. Update Vercel endpoints to load these trained weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "setup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7b057b5-be30-4247-a175-554ae03a60e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.12/dist-packages (0.2.65)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tensorflowjs in /usr/local/lib/python3.12/dist-packages (4.22.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.12/dist-packages (from yfinance) (0.0.12)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (4.4.0)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2025.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.12/dist-packages (from yfinance) (3.18.2)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.12/dist-packages (from yfinance) (4.13.5)\n",
            "Requirement already satisfied: curl_cffi>=0.7 in /usr/local/lib/python3.12/dist-packages (from yfinance) (0.13.0)\n",
            "Requirement already satisfied: websockets>=13.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (15.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: flax>=0.7.2 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs) (0.10.6)\n",
            "Requirement already satisfied: importlib_resources>=5.9.0 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs) (6.5.2)\n",
            "Requirement already satisfied: jax>=0.4.13 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs) (0.5.3)\n",
            "Requirement already satisfied: jaxlib>=0.4.13 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs) (0.5.3)\n",
            "Requirement already satisfied: tf-keras>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs) (2.19.0)\n",
            "Requirement already satisfied: tensorflow-decision-forests>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs) (1.12.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.16.1 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs) (0.16.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.8)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from curl_cffi>=0.7->yfinance) (1.17.1)\n",
            "Requirement already satisfied: certifi>=2024.2.2 in /usr/local/lib/python3.12/dist-packages (from curl_cffi>=0.7->yfinance) (2025.8.3)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.12/dist-packages (from flax>=0.7.2->tensorflowjs) (1.1.1)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.12/dist-packages (from flax>=0.7.2->tensorflowjs) (0.2.5)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.12/dist-packages (from flax>=0.7.2->tensorflowjs) (0.11.24)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.12/dist-packages (from flax>=0.7.2->tensorflowjs) (0.1.76)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.12/dist-packages (from flax>=0.7.2->tensorflowjs) (13.9.4)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.12/dist-packages (from flax>=0.7.2->tensorflowjs) (6.0.2)\n",
            "Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from flax>=0.7.2->tensorflowjs) (0.1.10)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: wurlitzer in /usr/local/lib/python3.12/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (3.1.1)\n",
            "Requirement already satisfied: ydf>=0.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (0.13.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.22)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (2.19.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.12/dist-packages (from optax->flax>=0.7.2->tensorflowjs) (0.1.90)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (1.13.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (1.6.0)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (24.1.0)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (4.13.0)\n",
            "Requirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (3.20.1)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax->flax>=0.7.2->tensorflowjs) (0.12.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.7.2->tensorflowjs) (0.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs) (2025.3.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs) (3.23.0)\n",
            "TensorFlow: 2.19.0\n",
            "GPU: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install tensorflow yfinance pandas numpy scikit-learn tensorflowjs\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import json\n",
        "import os\n",
        "\n",
        "print(f\"TensorFlow: {tf.__version__}\")\n",
        "print(f\"GPU: {tf.config.list_physical_devices('GPU')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data"
      },
      "source": [
        "## 1. Data Collection and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fetch_data",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08abb0c2-d454-4696-c2b4-ced4927508d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching market data...\n",
            "AAPL: 501 days\n",
            "MSFT: 501 days\n",
            "GOOGL: 501 days\n",
            "TSLA: 501 days\n",
            "NVDA: 501 days\n",
            "Total: 2505 data points\n"
          ]
        }
      ],
      "source": [
        "# Fetch real market data\n",
        "symbols = ['AAPL', 'MSFT', 'GOOGL', 'TSLA', 'NVDA']\n",
        "print(\"Fetching market data...\")\n",
        "\n",
        "all_data = []\n",
        "for symbol in symbols:\n",
        "    ticker = yf.Ticker(symbol)\n",
        "    data = ticker.history(period='2y', interval='1d')\n",
        "    data['Symbol'] = symbol\n",
        "    all_data.append(data)\n",
        "    print(f\"{symbol}: {len(data)} days\")\n",
        "\n",
        "combined_data = pd.concat(all_data)\n",
        "print(f\"Total: {len(combined_data)} data points\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "preprocess",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda7a5b1-0877-45be-ed1d-c2fab6838d9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: X=(2355, 30, 6), y=(2355,)\n",
            "Train: (1884, 30, 6), Test: (471, 30, 6)\n"
          ]
        }
      ],
      "source": [
        "# Create sequences for training\n",
        "def create_sequences(data, seq_length=30):\n",
        "    X, y = [], []\n",
        "\n",
        "    for symbol in data['Symbol'].unique():\n",
        "        symbol_data = data[data['Symbol'] == symbol].sort_index()\n",
        "\n",
        "        # Features: OHLCV + technical indicators\n",
        "        features = symbol_data[['Open', 'High', 'Low', 'Close', 'Volume']].values\n",
        "\n",
        "        # Add price changes\n",
        "        price_changes = np.diff(symbol_data['Close'].values, prepend=symbol_data['Close'].values[0])\n",
        "        price_changes = price_changes / symbol_data['Close'].values  # Percentage change\n",
        "\n",
        "        # Combine features\n",
        "        enhanced_features = np.column_stack([features, price_changes])\n",
        "\n",
        "        # Normalize\n",
        "        scaler = MinMaxScaler()\n",
        "        scaled_features = scaler.fit_transform(enhanced_features)\n",
        "\n",
        "        # Create sequences\n",
        "        for i in range(seq_length, len(scaled_features)):\n",
        "            X.append(scaled_features[i-seq_length:i])  # Past 30 days\n",
        "\n",
        "            # Target: next day price change\n",
        "            current_price = symbol_data.iloc[i-1]['Close']\n",
        "            next_price = symbol_data.iloc[i]['Close']\n",
        "            target = (next_price - current_price) / current_price\n",
        "            y.append(target)\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Create dataset\n",
        "X, y = create_sequences(combined_data)\n",
        "print(f\"Dataset shape: X={X.shape}, y={y.shape}\")\n",
        "\n",
        "# Train/test split\n",
        "split = int(0.8 * len(X))\n",
        "X_train, X_test = X[:split], X[split:]\n",
        "y_train, y_test = y[:split], y[split:]\n",
        "\n",
        "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "models"
      },
      "source": [
        "## 2. Model Architectures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tft_model",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "131f785e-321a-470d-bf30-5f89f22a4637"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TFT Model: 37,099 parameters\n"
          ]
        }
      ],
      "source": [
        "# TFT Model (simplified for training, with TF.js-compatible custom attention)\n",
        "def create_tft_model(input_shape):\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "\n",
        "    # Variable selection\n",
        "    context = tf.keras.layers.GlobalAveragePooling1D()(inputs)\n",
        "    selection_weights = tf.keras.layers.Dense(input_shape[-1], activation='softmax')(context)\n",
        "    selection_weights = tf.keras.layers.RepeatVector(input_shape[0])(selection_weights)\n",
        "    selected_features = tf.keras.layers.Multiply()([inputs, selection_weights])\n",
        "\n",
        "    # LSTM processing\n",
        "    lstm_out = tf.keras.layers.LSTM(64, return_sequences=True, dropout=0.2)(selected_features)\n",
        "\n",
        "    # Custom Multi-Head Attention (using supported operations)\n",
        "    embed_dim = lstm_out.shape[-1]\n",
        "    num_heads = 4\n",
        "    key_dim = 16  # Per-head dimension; total projection_dim = key_dim * num_heads = 64\n",
        "    projection_dim = key_dim * num_heads\n",
        "\n",
        "    # Linear projections for Query, Key, Value\n",
        "    query = tf.keras.layers.Dense(projection_dim)(lstm_out)\n",
        "    key = tf.keras.layers.Dense(projection_dim)(lstm_out)\n",
        "    value = tf.keras.layers.Dense(projection_dim)(lstm_out)\n",
        "\n",
        "    # Reshape for multi-head: (batch_size, seq_length, num_heads, key_dim)\n",
        "    batch_size = tf.shape(lstm_out)[0]\n",
        "    seq_len = tf.shape(lstm_out)[1]\n",
        "    query = tf.reshape(query, (batch_size, seq_len, num_heads, key_dim))\n",
        "    key = tf.reshape(key, (batch_size, seq_len, num_heads, key_dim))\n",
        "    value = tf.reshape(value, (batch_size, seq_len, num_heads, key_dim))\n",
        "    query = tf.transpose(query, perm=[0, 2, 1, 3])  # (batch_size, num_heads, seq_length, key_dim)\n",
        "    key = tf.transpose(key, perm=[0, 2, 1, 3])\n",
        "    value = tf.transpose(value, perm=[0, 2, 1, 3])\n",
        "\n",
        "    # Scaled dot-product attention\n",
        "    scores = tf.matmul(query, key, transpose_b=True) / tf.sqrt(tf.cast(key_dim, tf.float32))\n",
        "    weights = tf.nn.softmax(scores, axis=-1)\n",
        "    attention_output = tf.matmul(weights, value)\n",
        "\n",
        "    # Reshape back: (batch_size, seq_length, projection_dim)\n",
        "    attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])\n",
        "    attention_output = tf.reshape(attention_output, (batch_size, seq_len, projection_dim))\n",
        "\n",
        "    # Final projection to match embed_dim\n",
        "    attention = tf.keras.layers.Dense(embed_dim)(attention_output)\n",
        "\n",
        "    # Residual connection and normalization\n",
        "    attention = tf.keras.layers.LayerNormalization()(attention + lstm_out)\n",
        "\n",
        "    # Output\n",
        "    pooled = tf.keras.layers.GlobalAveragePooling1D()(attention)\n",
        "    dense = tf.keras.layers.Dense(32, activation='relu')(pooled)\n",
        "    output = tf.keras.layers.Dense(1)(dense)\n",
        "\n",
        "    model = tf.keras.Model(inputs, output, name='TFT')\n",
        "    return model\n",
        "\n",
        "tft_model = create_tft_model((X.shape[1], X.shape[2]))\n",
        "print(f\"TFT Model: {tft_model.count_params():,} parameters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nhits_model",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "672edffe-f43a-4c53-f3a4-3ead8fbb6943"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N-HITS Model: 108,289 parameters\n"
          ]
        }
      ],
      "source": [
        "# N-HITS Model (hierarchical blocks)\n",
        "def create_nhits_model(input_shape):\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "\n",
        "    # Input projection\n",
        "    x = tf.keras.layers.Dense(128)(inputs)\n",
        "\n",
        "    # Hierarchical stacks\n",
        "    stack_outputs = []\n",
        "\n",
        "    for pool_size in [2, 4, 8]:  # Different time scales\n",
        "        # Downsample\n",
        "        downsampled = tf.keras.layers.AveragePooling1D(pool_size, padding='same')(x)\n",
        "\n",
        "        # MLP blocks\n",
        "        mlp = tf.keras.layers.Dense(128, activation='relu')(downsampled)\n",
        "        mlp = tf.keras.layers.Dense(128, activation='relu')(mlp)\n",
        "\n",
        "        # Upsample back\n",
        "        upsampled = tf.keras.layers.UpSampling1D(pool_size)(mlp)\n",
        "\n",
        "        # Ensure same length as original\n",
        "        if upsampled.shape[1] != input_shape[0]:\n",
        "            upsampled = upsampled[:, :input_shape[0], :]\n",
        "\n",
        "        stack_outputs.append(upsampled)\n",
        "\n",
        "    # Combine hierarchical outputs\n",
        "    combined = tf.keras.layers.Add()(stack_outputs)\n",
        "\n",
        "    # Final prediction\n",
        "    pooled = tf.keras.layers.GlobalAveragePooling1D()(combined)\n",
        "    dense = tf.keras.layers.Dense(64, activation='relu')(pooled)\n",
        "    output = tf.keras.layers.Dense(1)(dense)\n",
        "\n",
        "    model = tf.keras.Model(inputs, output, name='NHITS')\n",
        "    return model\n",
        "\n",
        "nhits_model = create_nhits_model((X.shape[1], X.shape[2]))\n",
        "print(f\"N-HITS Model: {nhits_model.count_params():,} parameters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training"
      },
      "source": [
        "## 3. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "train_tft",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deb126c6-c104-43f8-e1ca-3c47e6b22599"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training TFT...\n",
            "Epoch 1/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - loss: 0.0558 - mae: 0.1557 - val_loss: 0.0012 - val_mae: 0.0256 - learning_rate: 0.0010\n",
            "Epoch 2/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 7.7275e-04 - mae: 0.0195 - val_loss: 0.0011 - val_mae: 0.0241 - learning_rate: 0.0010\n",
            "Epoch 3/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 6.1718e-04 - mae: 0.0177 - val_loss: 0.0011 - val_mae: 0.0243 - learning_rate: 0.0010\n",
            "Epoch 4/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 7.8145e-04 - mae: 0.0201 - val_loss: 0.0011 - val_mae: 0.0237 - learning_rate: 0.0010\n",
            "Epoch 5/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 7.4257e-04 - mae: 0.0192 - val_loss: 0.0017 - val_mae: 0.0320 - learning_rate: 0.0010\n",
            "Epoch 6/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 7.7216e-04 - mae: 0.0202 - val_loss: 0.0010 - val_mae: 0.0231 - learning_rate: 1.0000e-04\n",
            "Epoch 7/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 5.9414e-04 - mae: 0.0165 - val_loss: 0.0011 - val_mae: 0.0234 - learning_rate: 1.0000e-04\n",
            "Epoch 8/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 6.7949e-04 - mae: 0.0170 - val_loss: 0.0010 - val_mae: 0.0231 - learning_rate: 1.0000e-04\n",
            "Epoch 9/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 6.3715e-04 - mae: 0.0165 - val_loss: 0.0010 - val_mae: 0.0232 - learning_rate: 1.0000e-05\n",
            "Epoch 10/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 6.0426e-04 - mae: 0.0160 - val_loss: 0.0010 - val_mae: 0.0231 - learning_rate: 1.0000e-05\n",
            "Epoch 11/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 6.2237e-04 - mae: 0.0164 - val_loss: 0.0010 - val_mae: 0.0232 - learning_rate: 1.0000e-05\n",
            "Epoch 12/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 7.4163e-04 - mae: 0.0174 - val_loss: 0.0010 - val_mae: 0.0232 - learning_rate: 1.0000e-06\n",
            "Epoch 13/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 6.2378e-04 - mae: 0.0166 - val_loss: 0.0010 - val_mae: 0.0232 - learning_rate: 1.0000e-06\n",
            "Epoch 14/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 6.2870e-04 - mae: 0.0163 - val_loss: 0.0010 - val_mae: 0.0232 - learning_rate: 1.0000e-06\n",
            "Epoch 15/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 5.6758e-04 - mae: 0.0158 - val_loss: 0.0010 - val_mae: 0.0232 - learning_rate: 1.0000e-07\n",
            "TFT Training Complete\n"
          ]
        }
      ],
      "source": [
        "# Train TFT Model\n",
        "print(\"Training TFT...\")\n",
        "\n",
        "tft_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mse',\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "tft_history = tft_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    verbose=1,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(patience=3)\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"TFT Training Complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "train_nhits",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c30e210-f530-4306-98fa-ee94a30e373b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training N-HITS...\n",
            "Epoch 1/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 59ms/step - loss: 0.0092 - mae: 0.0651 - val_loss: 0.0011 - val_mae: 0.0236 - learning_rate: 0.0010\n",
            "Epoch 2/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 7.0707e-04 - mae: 0.0182 - val_loss: 0.0011 - val_mae: 0.0235 - learning_rate: 0.0010\n",
            "Epoch 3/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 6.3137e-04 - mae: 0.0167 - val_loss: 0.0013 - val_mae: 0.0275 - learning_rate: 0.0010\n",
            "Epoch 4/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 7.1727e-04 - mae: 0.0182 - val_loss: 0.0011 - val_mae: 0.0241 - learning_rate: 0.0010\n",
            "Epoch 5/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.4599e-04 - mae: 0.0172 - val_loss: 0.0010 - val_mae: 0.0231 - learning_rate: 1.0000e-04\n",
            "Epoch 6/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.0865e-04 - mae: 0.0160 - val_loss: 0.0011 - val_mae: 0.0237 - learning_rate: 1.0000e-04\n",
            "Epoch 7/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.4862e-04 - mae: 0.0164 - val_loss: 0.0011 - val_mae: 0.0233 - learning_rate: 1.0000e-04\n",
            "Epoch 8/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.4755e-04 - mae: 0.0159 - val_loss: 0.0011 - val_mae: 0.0234 - learning_rate: 1.0000e-05\n",
            "Epoch 9/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.5739e-04 - mae: 0.0158 - val_loss: 0.0011 - val_mae: 0.0233 - learning_rate: 1.0000e-05\n",
            "Epoch 10/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.5878e-04 - mae: 0.0168 - val_loss: 0.0011 - val_mae: 0.0234 - learning_rate: 1.0000e-05\n",
            "N-HITS Training Complete\n"
          ]
        }
      ],
      "source": [
        "# Train N-HITS Model\n",
        "print(\"Training N-HITS...\")\n",
        "\n",
        "nhits_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mse',\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "nhits_history = nhits_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    verbose=1,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(patience=3)\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"N-HITS Training Complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evaluation"
      },
      "source": [
        "## 4. Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "evaluate",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d72ff02d-0302-4884-8bda-2797d1a62737"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TFT - Loss: 0.001043, MAE: 0.023067\n",
            "N-HITS - Loss: 0.001046, MAE: 0.023077\n",
            "\n",
            "Direction Accuracy:\n",
            "TFT: 64.0%\n",
            "N-HITS: 59.0%\n"
          ]
        }
      ],
      "source": [
        "# Evaluate models\n",
        "tft_loss, tft_mae = tft_model.evaluate(X_test, y_test, verbose=0)\n",
        "nhits_loss, nhits_mae = nhits_model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"TFT - Loss: {tft_loss:.6f}, MAE: {tft_mae:.6f}\")\n",
        "print(f\"N-HITS - Loss: {nhits_loss:.6f}, MAE: {nhits_mae:.6f}\")\n",
        "\n",
        "# Direction accuracy\n",
        "tft_pred = tft_model.predict(X_test[:100], verbose=0)\n",
        "nhits_pred = nhits_model.predict(X_test[:100], verbose=0)\n",
        "actual = y_test[:100]\n",
        "\n",
        "tft_dir_acc = np.mean(np.sign(tft_pred.flatten()) == np.sign(actual))\n",
        "nhits_dir_acc = np.mean(np.sign(nhits_pred.flatten()) == np.sign(actual))\n",
        "\n",
        "print(f\"\\nDirection Accuracy:\")\n",
        "print(f\"TFT: {tft_dir_acc:.1%}\")\n",
        "print(f\"N-HITS: {nhits_dir_acc:.1%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "export"
      },
      "source": [
        "## 5. Export for Vercel Deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "export_models",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "b79d4cf7-ac44-46a9-a437-3c65abcc5d66"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<p style=\"margin:0px;\">ğŸŒ² Try <a href=\"https://ydf.readthedocs.io/en/latest/\" target=\"_blank\">YDF</a>, the successor of\n",
              "    <a href=\"https://www.tensorflow.org/decision_forests\" target=\"_blank\">TensorFlow\n",
              "        Decision Forests</a> using the same algorithms but with more features and faster\n",
              "    training!\n",
              "</p>\n",
              "<div style=\"display: flex; flex-wrap: wrap; margin:5px;max-width: 880px;\">\n",
              "    <div style=\"flex: 1; border-radius: 10px; background-color: F0F0F0; padding: 5px;\">\n",
              "        <p\n",
              "            style=\"font-weight: bold; margin:0px;text-align: center;border-bottom: 1px solid #C0C0C0;margin-bottom: 4px;\">\n",
              "            Old code</p>\n",
              "        <pre style=\"overflow-wrap: anywhere; overflow: auto; margin:0px;font-size: 9pt;\">\n",
              "import tensorflow_decision_forests as tfdf\n",
              "\n",
              "tf_ds = tfdf.keras.pd_dataframe_to_tf_dataset(ds, label=\"l\")\n",
              "model = tfdf.keras.RandomForestModel(label=\"l\")\n",
              "model.fit(tf_ds)\n",
              "</pre>\n",
              "    </div>\n",
              "    <div style=\"width: 5px;\"></div>\n",
              "    <div style=\"flex: 1; border-radius: 10px; background-color: F0F0F0; padding: 5px;\">\n",
              "        <p\n",
              "            style=\"font-weight: bold; margin:0px;text-align: center;border-bottom: 1px solid #C0C0C0;margin-bottom: 4px;\">\n",
              "            New code</p>\n",
              "        <pre style=\"overflow-wrap: anywhere; overflow: auto; margin:0px;font-size: 9pt;\">\n",
              "import ydf\n",
              "\n",
              "model = ydf.RandomForestLearner(label=\"l\").train(ds)\n",
              "</pre>\n",
              "    </div>\n",
              "</div>\n",
              "<p style=\"margin:0px;font-size: 9pt;\">(Learn more in the <a\n",
              "        href=\"https://ydf.readthedocs.io/en/latest/tutorial/migrating_to_ydf/\" target=\"_blank\">migration\n",
              "        guide</a>)</p>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exporting trained models...\n",
            "failed to lookup keras version from the file,\n",
            "    this is likely a weight only file\n",
            "failed to lookup keras version from the file,\n",
            "    this is likely a weight only file\n",
            "Models exported to TensorFlow.js format\n",
            "Metadata saved\n"
          ]
        }
      ],
      "source": [
        "import tensorflowjs as tfjs\n",
        "import zipfile\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Create export directory\n",
        "os.makedirs('/content/models', exist_ok=True)\n",
        "\n",
        "print(\"Exporting trained models...\")\n",
        "\n",
        "# Save models in native Keras format (.keras) to avoid HDF5 warnings\n",
        "tft_model.save('/content/models/tft-trained.keras')\n",
        "nhits_model.save('/content/models/nhits-trained.keras')\n",
        "\n",
        "# Convert to TensorFlow.js format using CLI for better compatibility\n",
        "!tensorflowjs_converter --input_format=keras /content/models/tft-trained.keras /content/models/tft-trained\n",
        "!tensorflowjs_converter --input_format=keras /content/models/nhits-trained.keras /content/models/nhits-trained\n",
        "\n",
        "# Optional: Quantize for smaller size (add --quantize_float16 if needed for Cloudflare limits)\n",
        "# !tensorflowjs_converter --input_format=keras --quantize_float16 /content/models/tft-trained.keras /content/models/tft-trained\n",
        "\n",
        "print(\"Models exported to TensorFlow.js format\")\n",
        "\n",
        "# Save model metadata (unchanged)\n",
        "metadata = {\n",
        "    'tft': {\n",
        "        'loss': float(tft_loss),\n",
        "        'mae': float(tft_mae),\n",
        "        'direction_accuracy': float(tft_dir_acc),\n",
        "        'parameters': int(tft_model.count_params())\n",
        "    },\n",
        "    'nhits': {\n",
        "        'loss': float(nhits_loss),\n",
        "        'mae': float(nhits_mae),\n",
        "        'direction_accuracy': float(nhits_dir_acc),\n",
        "        'parameters': int(nhits_model.count_params())\n",
        "    },\n",
        "    'training_info': {\n",
        "        'sequence_length': int(X.shape[1]),\n",
        "        'num_features': int(X.shape[2]),\n",
        "        'training_samples': int(len(X_train)),\n",
        "        'test_samples': int(len(X_test)),\n",
        "        'symbols': symbols\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('/content/models/metadata.json', 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(\"Metadata saved\")\n",
        "\n",
        "# The rest (create_zip, etc.) remains unchanged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "create_zip",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6835a6f0-3b7d-4a84-b83a-a7538db5a384"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created trained_models.zip\n",
            "\n",
            "Package contents:\n",
            "  models/metadata.json\n",
            "  models/tft-trained/group1-shard1of1.bin\n",
            "  models/tft-trained/model.json\n",
            "  models/nhits-trained/group1-shard1of1.bin\n",
            "  models/nhits-trained/model.json\n",
            "\n",
            "Package size: 0.6 MB\n"
          ]
        }
      ],
      "source": [
        "# Create deployment package\n",
        "def create_zip():\n",
        "    with zipfile.ZipFile('/content/trained_models.zip', 'w') as zipf:\n",
        "        for root, dirs, files in os.walk('/content/models'):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                arcname = os.path.relpath(file_path, '/content')\n",
        "                zipf.write(file_path, arcname)\n",
        "\n",
        "create_zip()\n",
        "print(\"Created trained_models.zip\")\n",
        "\n",
        "# Show what's in the package\n",
        "print(\"\\nPackage contents:\")\n",
        "with zipfile.ZipFile('/content/trained_models.zip', 'r') as zipf:\n",
        "    for name in zipf.namelist():\n",
        "        print(f\"  {name}\")\n",
        "\n",
        "print(f\"\\nPackage size: {os.path.getsize('/content/trained_models.zip') / 1024 / 1024:.1f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "download",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "142aebb5-2407-4b7f-d17c-f1a513f98a0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading trained models...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_434f2b40-74b3-44e2-847a-179d1a38c7bf\", \"trained_models.zip\", 607953)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ‰ Training Complete!\n",
            "\n",
            "Next steps:\n",
            "1. Extract trained_models.zip\n",
            "2. Upload models/ folder to your Vercel project\n",
            "3. Update predict-tft.js and predict-nhits.js to load these models:\n",
            "   await tf.loadLayersModel('./models/tft-trained/model.json')\n",
            "   await tf.loadLayersModel('./models/nhits-trained/model.json')\n",
            "4. Deploy updated Vercel project\n",
            "\n",
            "Model Performance:\n",
            "TFT: 64.0% direction accuracy, 0.023067 MAE\n",
            "N-HITS: 59.0% direction accuracy, 0.023077 MAE\n"
          ]
        }
      ],
      "source": [
        "# Download the trained models\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Downloading trained models...\")\n",
        "files.download('/content/trained_models.zip')\n",
        "\n",
        "print(\"\\nğŸ‰ Training Complete!\")\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"1. Extract trained_models.zip\")\n",
        "print(\"2. Upload models/ folder to your Vercel project\")\n",
        "print(\"3. Update predict-tft.js and predict-nhits.js to load these models:\")\n",
        "print(\"   await tf.loadLayersModel('./models/tft-trained/model.json')\")\n",
        "print(\"   await tf.loadLayersModel('./models/nhits-trained/model.json')\")\n",
        "print(\"4. Deploy updated Vercel project\")\n",
        "\n",
        "print(f\"\\nModel Performance:\")\n",
        "print(f\"TFT: {tft_dir_acc:.1%} direction accuracy, {tft_mae:.6f} MAE\")\n",
        "print(f\"N-HITS: {nhits_dir_acc:.1%} direction accuracy, {nhits_mae:.6f} MAE\")"
      ]
    }
  ]
}