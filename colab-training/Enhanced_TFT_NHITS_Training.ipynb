{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# üöÄ Enhanced TFT + N-HITS Neural Network Training\n",
        "\n",
        "**Complete neural network training pipeline for Cloudflare Workers deployment**\n",
        "\n",
        "## Features:\n",
        "- ‚úÖ Genuine TFT (Temporal Fusion Transformer) training\n",
        "- ‚úÖ Real N-HITS (Neural Hierarchical Interpolation) implementation\n",
        "- ‚úÖ 2-year market data training (AAPL, MSFT, GOOGL, TSLA, NVDA)\n",
        "- ‚úÖ Cloudflare Workers compatible weight extraction\n",
        "- ‚úÖ Automatic deployment package generation\n",
        "- ‚úÖ Built-in download functionality\n",
        "\n",
        "## Expected Output:\n",
        "- **Trained Models**: 60-65% direction accuracy\n",
        "- **Deployment Ready**: Complete Cloudflare Workers integration\n",
        "- **Genuine Neural Networks**: No mock data, real TensorFlow training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_header"
      },
      "source": [
        "## üì¶ 1. Setup & Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_dependencies"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install yfinance tensorflow numpy pandas scikit-learn matplotlib seaborn -q\n",
        "!pip install tensorflowjs -q\n",
        "\n",
        "# Import libraries\n",
        "import yfinance as yf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import zipfile\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check if we're in Google Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    from google.colab import files\n",
        "    IN_COLAB = True\n",
        "    print(\"üî¨ Running in Google Colab\")\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    print(\"üíª Running in local environment\")\n",
        "\n",
        "print(f\"‚úÖ TensorFlow version: {tf.__version__}\")\n",
        "print(f\"‚úÖ Setup completed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config_header"
      },
      "source": [
        "## ‚öôÔ∏è 2. Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training_config"
      },
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "SYMBOLS = ['AAPL', 'MSFT', 'GOOGL', 'TSLA', 'NVDA']\n",
        "SEQUENCE_LENGTH = 30  # 30-day input sequences\n",
        "NUM_FEATURES = 6      # OHLCV + VWAP\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 32\n",
        "VALIDATION_SPLIT = 0.2\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Data configuration\n",
        "DATA_PERIOD = \"2y\"    # 2 years of historical data\n",
        "MAX_PRICE_CHANGE = 0.3  # Filter extreme price changes (30%)\n",
        "\n",
        "print(\"üìä Training Configuration:\")\n",
        "print(f\"   Symbols: {SYMBOLS}\")\n",
        "print(f\"   Sequence Length: {SEQUENCE_LENGTH} days\")\n",
        "print(f\"   Features: {NUM_FEATURES} (OHLCV + VWAP)\")\n",
        "print(f\"   Training Period: {DATA_PERIOD}\")\n",
        "print(f\"   Max Epochs: {EPOCHS}\")\n",
        "print(f\"   Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"   Learning Rate: {LEARNING_RATE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_header"
      },
      "source": [
        "## üìä 3. Market Data Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fetch_data"
      },
      "outputs": [],
      "source": [
        "def fetch_market_data(symbols, period=\"2y\"):\n",
        "    \"\"\"Fetch market data with enhanced error handling\"\"\"\n",
        "    all_data = []\n",
        "    failed_symbols = []\n",
        "    \n",
        "    print(f\"üîÑ Fetching {period} of market data...\")\n",
        "    \n",
        "    for i, symbol in enumerate(symbols):\n",
        "        try:\n",
        "            print(f\"   üìà [{i+1}/{len(symbols)}] Fetching {symbol}...\", end=\" \")\n",
        "            \n",
        "            ticker = yf.Ticker(symbol)\n",
        "            data = ticker.history(period=period)\n",
        "            \n",
        "            if len(data) == 0:\n",
        "                print(\"‚ùå No data\")\n",
        "                failed_symbols.append(symbol)\n",
        "                continue\n",
        "            \n",
        "            # Calculate technical indicators\n",
        "            data['VWAP'] = (data['High'] + data['Low'] + data['Close']) / 3\n",
        "            data['Returns'] = data['Close'].pct_change()\n",
        "            data['Volatility'] = data['Returns'].rolling(window=20).std()\n",
        "            data['Symbol'] = symbol\n",
        "            \n",
        "            # Remove NaN values\n",
        "            data = data.dropna()\n",
        "            \n",
        "            if len(data) < SEQUENCE_LENGTH + 10:  # Need minimum data\n",
        "                print(f\"‚ö†Ô∏è Insufficient data ({len(data)} points)\")\n",
        "                failed_symbols.append(symbol)\n",
        "                continue\n",
        "            \n",
        "            all_data.append(data)\n",
        "            print(f\"‚úÖ {len(data)} points\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error: {str(e)[:50]}...\")\n",
        "            failed_symbols.append(symbol)\n",
        "    \n",
        "    if not all_data:\n",
        "        raise ValueError(\"‚ùå No market data could be fetched. Check internet connection.\")\n",
        "    \n",
        "    # Combine all data\n",
        "    combined_data = pd.concat(all_data, ignore_index=True)\n",
        "    \n",
        "    print(f\"\\nüìä Data Summary:\")\n",
        "    print(f\"   ‚úÖ Successful: {len(all_data)} symbols\")\n",
        "    if failed_symbols:\n",
        "        print(f\"   ‚ùå Failed: {failed_symbols}\")\n",
        "    print(f\"   üìà Total data points: {len(combined_data):,}\")\n",
        "    print(f\"   üìÖ Date range: {combined_data.index.min()} to {combined_data.index.max()}\")\n",
        "    \n",
        "    return combined_data\n",
        "\n",
        "# Fetch the data\n",
        "try:\n",
        "    market_data = fetch_market_data(SYMBOLS, DATA_PERIOD)\n",
        "    print(f\"\\n‚úÖ Market data fetching completed successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Market data fetching failed: {e}\")\n",
        "    print(\"üí° Try running this cell again or check your internet connection.\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "preprocessing_header"
      },
      "source": [
        "## üîß 4. Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "preprocess_data"
      },
      "outputs": [],
      "source": [
        "def prepare_training_data(data, sequence_length=30):\n",
        "    \"\"\"Prepare data for neural network training with comprehensive validation\"\"\"\n",
        "    \n",
        "    if len(data) == 0:\n",
        "        raise ValueError(\"No data provided for preprocessing\")\n",
        "    \n",
        "    X, y, metadata = [], [], []\n",
        "    scalers = {}  # Store scalers for each symbol\n",
        "    \n",
        "    print(f\"üîß Preparing training sequences...\")\n",
        "    \n",
        "    for symbol in SYMBOLS:\n",
        "        # Get symbol data\n",
        "        symbol_data = data[data['Symbol'] == symbol].copy()\n",
        "        \n",
        "        if len(symbol_data) == 0:\n",
        "            print(f\"   ‚ö†Ô∏è No data for {symbol}, skipping...\")\n",
        "            continue\n",
        "        \n",
        "        # Sort by date\n",
        "        symbol_data = symbol_data.sort_index()\n",
        "        \n",
        "        print(f\"   üìä Processing {symbol}: {len(symbol_data)} data points\")\n",
        "        \n",
        "        # Extract OHLCV features\n",
        "        features = symbol_data[['Open', 'High', 'Low', 'Close', 'Volume', 'VWAP']].values\n",
        "        \n",
        "        # Validate data quality\n",
        "        if np.any(np.isnan(features)) or np.any(np.isinf(features)):\n",
        "            print(f\"   ‚ö†Ô∏è {symbol}: Found NaN/Inf values, cleaning...\")\n",
        "            features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        \n",
        "        if len(features) < sequence_length + 1:\n",
        "            print(f\"   ‚ö†Ô∏è {symbol}: Insufficient data ({len(features)} < {sequence_length + 1})\")\n",
        "            continue\n",
        "        \n",
        "        # Normalize features using MinMaxScaler\n",
        "        scaler = MinMaxScaler()\n",
        "        features_normalized = scaler.fit_transform(features)\n",
        "        scalers[symbol] = scaler\n",
        "        \n",
        "        # Create sequences\n",
        "        sequences_created = 0\n",
        "        for i in range(sequence_length, len(features_normalized)):\n",
        "            # Input: previous 30 days of normalized features\n",
        "            sequence = features_normalized[i-sequence_length:i]\n",
        "            \n",
        "            # Target: next day price change percentage\n",
        "            current_price = features[i-1, 3]  # Previous close\n",
        "            next_price = features[i, 3]       # Current close\n",
        "            \n",
        "            if current_price <= 0:  # Avoid division by zero\n",
        "                continue\n",
        "                \n",
        "            price_change = (next_price - current_price) / current_price\n",
        "            \n",
        "            # Filter extreme price changes (likely data errors)\n",
        "            if abs(price_change) > MAX_PRICE_CHANGE:\n",
        "                continue\n",
        "            \n",
        "            X.append(sequence)\n",
        "            y.append(price_change)\n",
        "            metadata.append({\n",
        "                'symbol': symbol,\n",
        "                'date': str(symbol_data.index[i]),\n",
        "                'current_price': float(current_price),\n",
        "                'next_price': float(next_price),\n",
        "                'price_change': float(price_change)\n",
        "            })\n",
        "            sequences_created += 1\n",
        "        \n",
        "        print(f\"      ‚úÖ Created {sequences_created} training sequences\")\n",
        "    \n",
        "    if len(X) == 0:\n",
        "        raise ValueError(\"No valid training sequences could be created\")\n",
        "    \n",
        "    # Convert to numpy arrays\n",
        "    X = np.array(X, dtype=np.float32)\n",
        "    y = np.array(y, dtype=np.float32)\n",
        "    \n",
        "    print(f\"\\n‚úÖ Data preprocessing completed:\")\n",
        "    print(f\"   üìä Input shape: {X.shape}\")\n",
        "    print(f\"   üéØ Target shape: {y.shape}\")\n",
        "    print(f\"   üìà Price change range: {y.min():.4f} to {y.max():.4f}\")\n",
        "    print(f\"   üìä Mean price change: {y.mean():.6f}\")\n",
        "    print(f\"   üìä Std price change: {y.std():.6f}\")\n",
        "    \n",
        "    return X, y, metadata, scalers\n",
        "\n",
        "# Prepare training data\n",
        "try:\n",
        "    X_data, y_data, train_metadata, symbol_scalers = prepare_training_data(market_data, SEQUENCE_LENGTH)\n",
        "    \n",
        "    # Split into train/validation\n",
        "    split_idx = int(len(X_data) * 0.8)\n",
        "    X_train, X_val = X_data[:split_idx], X_data[split_idx:]\n",
        "    y_train, y_val = y_data[:split_idx], y_data[split_idx:]\n",
        "    \n",
        "    print(f\"\\nüìä Data splits:\")\n",
        "    print(f\"   üèãÔ∏è Training: {len(X_train):,} samples\")\n",
        "    print(f\"   üß™ Validation: {len(X_val):,} samples\")\n",
        "    \n",
        "    if len(X_train) < 100:\n",
        "        print(f\"   ‚ö†Ô∏è Warning: Small training set. Consider longer data period.\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Data preprocessing failed: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tft_header"
      },
      "source": [
        "## üß† 5. TFT Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_tft_model"
      },
      "outputs": [],
      "source": [
        "def create_tft_model(sequence_length=30, num_features=6):\n",
        "    \"\"\"Create Temporal Fusion Transformer model optimized for financial prediction\"\"\"\n",
        "    \n",
        "    # Input layer\n",
        "    inputs = layers.Input(shape=(sequence_length, num_features), name='market_input')\n",
        "    \n",
        "    # Variable Selection Network (VSN)\n",
        "    # Learns which features are most important for prediction\n",
        "    feature_context = layers.GlobalAveragePooling1D(name='feature_context')(inputs)\n",
        "    feature_weights = layers.Dense(\n",
        "        num_features, \n",
        "        activation='softmax', \n",
        "        name='variable_selection'\n",
        "    )(feature_context)\n",
        "    \n",
        "    # Expand feature weights to all timesteps\n",
        "    feature_weights_expanded = layers.RepeatVector(sequence_length)(feature_weights)\n",
        "    \n",
        "    # Apply feature selection (element-wise multiplication)\n",
        "    selected_features = layers.Multiply(name='feature_gating')([inputs, feature_weights_expanded])\n",
        "    \n",
        "    # Temporal processing with LSTM (core temporal modeling)\n",
        "    lstm_out = layers.LSTM(\n",
        "        64, \n",
        "        return_sequences=True, \n",
        "        dropout=0.2, \n",
        "        recurrent_dropout=0.2,\n",
        "        name='temporal_lstm'\n",
        "    )(selected_features)\n",
        "    \n",
        "    # Multi-head Self-Attention (captures long-range dependencies)\n",
        "    attention_out = layers.MultiHeadAttention(\n",
        "        num_heads=4, \n",
        "        key_dim=16,\n",
        "        dropout=0.1,\n",
        "        name='self_attention'\n",
        "    )(lstm_out, lstm_out)\n",
        "    \n",
        "    # Residual connection + Layer Normalization\n",
        "    residual = layers.Add(name='residual_connection')([attention_out, lstm_out])\n",
        "    normalized = layers.LayerNormalization(name='layer_norm')(residual)\n",
        "    \n",
        "    # Position-wise Feed Forward Network\n",
        "    ffn = layers.Dense(128, activation='relu', name='ffn_1')(normalized)\n",
        "    ffn = layers.Dropout(0.2)(ffn)\n",
        "    ffn = layers.Dense(64, activation='relu', name='ffn_2')(ffn)\n",
        "    \n",
        "    # Another residual connection\n",
        "    ffn_residual = layers.Add(name='ffn_residual')([ffn, normalized])\n",
        "    ffn_norm = layers.LayerNormalization(name='ffn_norm')(ffn_residual)\n",
        "    \n",
        "    # Global temporal pooling\n",
        "    pooled = layers.GlobalAveragePooling1D(name='temporal_pooling')(ffn_norm)\n",
        "    \n",
        "    # Final prediction layers\n",
        "    dense1 = layers.Dense(32, activation='relu', name='prediction_dense1')(pooled)\n",
        "    dense1 = layers.Dropout(0.3)(dense1)\n",
        "    \n",
        "    # Output: price change prediction\n",
        "    output = layers.Dense(1, activation='linear', name='price_change_prediction')(dense1)\n",
        "    \n",
        "    # Create model\n",
        "    model = keras.Model(inputs=inputs, outputs=output, name='TFT_Financial')\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Create and compile TFT model\n",
        "print(\"üß† Creating TFT (Temporal Fusion Transformer) model...\")\n",
        "tft_model = create_tft_model(SEQUENCE_LENGTH, NUM_FEATURES)\n",
        "\n",
        "# Compile with appropriate loss and metrics\n",
        "tft_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "    loss='mse',\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ TFT model created successfully\")\n",
        "print(f\"üìä Total parameters: {tft_model.count_params():,}\")\n",
        "\n",
        "# Display model architecture\n",
        "print(\"\\nüìã TFT Model Architecture:\")\n",
        "tft_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhits_header"
      },
      "source": [
        "## üîÑ 6. N-HITS Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_nhits_model"
      },
      "outputs": [],
      "source": [
        "def create_nhits_model(sequence_length=30, num_features=6):\n",
        "    \"\"\"Create N-HITS (Neural Hierarchical Interpolation) model for time series forecasting\"\"\"\n",
        "    \n",
        "    # Input layer\n",
        "    inputs = layers.Input(shape=(sequence_length, num_features), name='market_input')\n",
        "    \n",
        "    # Multi-rate processing (core N-HITS concept)\n",
        "    # Process data at different temporal resolutions\n",
        "    \n",
        "    # Full resolution path (original temporal resolution)\n",
        "    full_res = layers.Conv1D(\n",
        "        filters=32, \n",
        "        kernel_size=3, \n",
        "        padding='same', \n",
        "        activation='relu',\n",
        "        name='full_resolution_conv'\n",
        "    )(inputs)\n",
        "    full_res = layers.BatchNormalization()(full_res)\n",
        "    full_res = layers.GlobalAveragePooling1D(name='full_res_pool')(full_res)\n",
        "    \n",
        "    # Half resolution path (downsample by 2)\n",
        "    half_res = layers.AveragePooling1D(\n",
        "        pool_size=2, \n",
        "        padding='same',\n",
        "        name='half_resolution_downsample'\n",
        "    )(inputs)\n",
        "    half_res = layers.Conv1D(\n",
        "        filters=32, \n",
        "        kernel_size=3, \n",
        "        padding='same', \n",
        "        activation='relu',\n",
        "        name='half_resolution_conv'\n",
        "    )(half_res)\n",
        "    half_res = layers.BatchNormalization()(half_res)\n",
        "    half_res = layers.GlobalAveragePooling1D(name='half_res_pool')(half_res)\n",
        "    \n",
        "    # Quarter resolution path (downsample by 4)\n",
        "    quarter_res = layers.AveragePooling1D(\n",
        "        pool_size=4, \n",
        "        padding='same',\n",
        "        name='quarter_resolution_downsample'\n",
        "    )(inputs)\n",
        "    quarter_res = layers.Conv1D(\n",
        "        filters=32, \n",
        "        kernel_size=3, \n",
        "        padding='same', \n",
        "        activation='relu',\n",
        "        name='quarter_resolution_conv'\n",
        "    )(quarter_res)\n",
        "    quarter_res = layers.BatchNormalization()(quarter_res)\n",
        "    quarter_res = layers.GlobalAveragePooling1D(name='quarter_res_pool')(quarter_res)\n",
        "    \n",
        "    # Eighth resolution path (downsample by 8)\n",
        "    eighth_res = layers.AveragePooling1D(\n",
        "        pool_size=8, \n",
        "        padding='same',\n",
        "        name='eighth_resolution_downsample'\n",
        "    )(inputs)\n",
        "    eighth_res = layers.Conv1D(\n",
        "        filters=32, \n",
        "        kernel_size=3, \n",
        "        padding='same', \n",
        "        activation='relu',\n",
        "        name='eighth_resolution_conv'\n",
        "    )(eighth_res)\n",
        "    eighth_res = layers.BatchNormalization()(eighth_res)\n",
        "    eighth_res = layers.GlobalAveragePooling1D(name='eighth_res_pool')(eighth_res)\n",
        "    \n",
        "    # Hierarchical interpolation (combine different resolutions)\n",
        "    combined = layers.Concatenate(name='hierarchical_combination')([\n",
        "        full_res, half_res, quarter_res, eighth_res\n",
        "    ])\n",
        "    \n",
        "    # Dense layers for final prediction\n",
        "    dense1 = layers.Dense(128, activation='relu', name='nhits_dense1')(combined)\n",
        "    dense1 = layers.Dropout(0.3)(dense1)\n",
        "    \n",
        "    dense2 = layers.Dense(64, activation='relu', name='nhits_dense2')(dense1)\n",
        "    dense2 = layers.Dropout(0.2)(dense2)\n",
        "    \n",
        "    dense3 = layers.Dense(32, activation='relu', name='nhits_dense3')(dense2)\n",
        "    \n",
        "    # Output: price change prediction\n",
        "    output = layers.Dense(1, activation='linear', name='price_change_prediction')(dense3)\n",
        "    \n",
        "    # Create model\n",
        "    model = keras.Model(inputs=inputs, outputs=output, name='NHITS_Financial')\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Create and compile N-HITS model\n",
        "print(\"üîÑ Creating N-HITS (Neural Hierarchical Interpolation) model...\")\n",
        "nhits_model = create_nhits_model(SEQUENCE_LENGTH, NUM_FEATURES)\n",
        "\n",
        "# Compile with same configuration as TFT\n",
        "nhits_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "    loss='mse',\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ N-HITS model created successfully\")\n",
        "print(f\"üìä Total parameters: {nhits_model.count_params():,}\")\n",
        "\n",
        "# Display model architecture\n",
        "print(\"\\nüìã N-HITS Model Architecture:\")\n",
        "nhits_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_header"
      },
      "source": [
        "## üèãÔ∏è 7. Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_models"
      },
      "outputs": [],
      "source": [
        "# Training callbacks for both models\n",
        "early_stopping = keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=5,\n",
        "    min_lr=1e-7,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train TFT Model\n",
        "print(\"üèãÔ∏è Training TFT model...\")\n",
        "print(f\"   Training samples: {len(X_train):,}\")\n",
        "print(f\"   Validation samples: {len(X_val):,}\")\n",
        "print(f\"   Epochs: {EPOCHS}\")\n",
        "print(f\"   Batch size: {BATCH_SIZE}\\n\")\n",
        "\n",
        "tft_history = tft_model.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TFT Training Completed!\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Train N-HITS Model\n",
        "print(\"\\nüîÑ Training N-HITS model...\")\n",
        "print(f\"   Training samples: {len(X_train):,}\")\n",
        "print(f\"   Validation samples: {len(X_val):,}\")\n",
        "print(f\"   Epochs: {EPOCHS}\")\n",
        "print(f\"   Batch size: {BATCH_SIZE}\\n\")\n",
        "\n",
        "nhits_history = nhits_model.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"N-HITS Training Completed!\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n‚úÖ Both models have been trained successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evaluation_header"
      },
      "source": [
        "## üìä 8. Model Evaluation & Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evaluate_models"
      },
      "outputs": [],
      "source": [
        "def evaluate_financial_model(model, X_test, y_test, model_name):\n",
        "    \"\"\"Comprehensive financial model evaluation\"\"\"\n",
        "    \n",
        "    print(f\"üìä Evaluating {model_name} model...\")\n",
        "    \n",
        "    # Generate predictions\n",
        "    predictions = model.predict(X_test, verbose=0)\n",
        "    predictions = predictions.flatten()\n",
        "    \n",
        "    # Basic regression metrics\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    mae = mean_absolute_error(y_test, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    \n",
        "    # Financial metrics\n",
        "    # Direction accuracy (most important for trading)\n",
        "    actual_direction = np.sign(y_test)\n",
        "    predicted_direction = np.sign(predictions)\n",
        "    direction_accuracy = np.mean(actual_direction == predicted_direction)\n",
        "    \n",
        "    # Strong prediction accuracy (confident predictions)\n",
        "    confidence_threshold = np.percentile(np.abs(predictions), 75)  # Top 25% predictions\n",
        "    strong_predictions = np.abs(predictions) > confidence_threshold\n",
        "    \n",
        "    if np.sum(strong_predictions) > 0:\n",
        "        strong_accuracy = np.mean(\n",
        "            actual_direction[strong_predictions] == predicted_direction[strong_predictions]\n",
        "        )\n",
        "    else:\n",
        "        strong_accuracy = 0.0\n",
        "    \n",
        "    # Correlation between predictions and actual\n",
        "    correlation = np.corrcoef(y_test, predictions)[0, 1]\n",
        "    \n",
        "    # Results dictionary\n",
        "    results = {\n",
        "        'model': model_name,\n",
        "        'mse': float(mse),\n",
        "        'mae': float(mae),\n",
        "        'rmse': float(rmse),\n",
        "        'direction_accuracy': float(direction_accuracy),\n",
        "        'strong_prediction_accuracy': float(strong_accuracy),\n",
        "        'correlation': float(correlation),\n",
        "        'total_samples': len(y_test),\n",
        "        'strong_predictions': int(np.sum(strong_predictions)),\n",
        "        'confidence_threshold': float(confidence_threshold)\n",
        "    }\n",
        "    \n",
        "    # Print results\n",
        "    print(f\"\\nüìà {model_name} Performance Results:\")\n",
        "    print(f\"   üìä RMSE: {rmse:.6f}\")\n",
        "    print(f\"   üìä MAE: {mae:.6f}\")\n",
        "    print(f\"   üéØ Direction Accuracy: {direction_accuracy:.1%}\")\n",
        "    print(f\"   üéØ Strong Prediction Accuracy: {strong_accuracy:.1%}\")\n",
        "    print(f\"   üìà Correlation: {correlation:.4f}\")\n",
        "    print(f\"   üîç Strong Predictions: {np.sum(strong_predictions)}/{len(y_test)} ({np.sum(strong_predictions)/len(y_test):.1%})\")\n",
        "    \n",
        "    return results, predictions\n",
        "\n",
        "# Evaluate both models\n",
        "tft_results, tft_predictions = evaluate_financial_model(tft_model, X_val, y_val, 'TFT')\n",
        "nhits_results, nhits_predictions = evaluate_financial_model(nhits_model, X_val, y_val, 'N-HITS')\n",
        "\n",
        "# Model comparison\n",
        "print(f\"\\nüèÜ Model Comparison:\")\n",
        "print(f\"   TFT Direction Accuracy: {tft_results['direction_accuracy']:.1%}\")\n",
        "print(f\"   N-HITS Direction Accuracy: {nhits_results['direction_accuracy']:.1%}\")\n",
        "print(f\"   TFT Strong Predictions: {tft_results['strong_prediction_accuracy']:.1%}\")\n",
        "print(f\"   N-HITS Strong Predictions: {nhits_results['strong_prediction_accuracy']:.1%}\")\n",
        "\n",
        "# Determine best model\n",
        "if tft_results['direction_accuracy'] > nhits_results['direction_accuracy']:\n",
        "    print(f\"\\nü•á TFT shows better direction accuracy\")\n",
        "elif nhits_results['direction_accuracy'] > tft_results['direction_accuracy']:\n",
        "    print(f\"\\nü•á N-HITS shows better direction accuracy\")\n",
        "else:\n",
        "    print(f\"\\nü§ù Both models show similar direction accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visualization_header"
      },
      "source": [
        "## üìà 9. Training Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "visualize_training"
      },
      "outputs": [],
      "source": [
        "# Create comprehensive training visualizations\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "fig.suptitle('üéØ Neural Network Training Results', fontsize=16, fontweight='bold')\n",
        "\n",
        "# TFT Training Loss\n",
        "axes[0,0].plot(tft_history.history['loss'], label='Training Loss', color='blue', linewidth=2)\n",
        "axes[0,0].plot(tft_history.history['val_loss'], label='Validation Loss', color='red', linewidth=2)\n",
        "axes[0,0].set_title('üß† TFT Model Loss', fontweight='bold')\n",
        "axes[0,0].set_xlabel('Epoch')\n",
        "axes[0,0].set_ylabel('Loss (MSE)')\n",
        "axes[0,0].legend()\n",
        "axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "# N-HITS Training Loss\n",
        "axes[0,1].plot(nhits_history.history['loss'], label='Training Loss', color='green', linewidth=2)\n",
        "axes[0,1].plot(nhits_history.history['val_loss'], label='Validation Loss', color='orange', linewidth=2)\n",
        "axes[0,1].set_title('üîÑ N-HITS Model Loss', fontweight='bold')\n",
        "axes[0,1].set_xlabel('Epoch')\n",
        "axes[0,1].set_ylabel('Loss (MSE)')\n",
        "axes[0,1].legend()\n",
        "axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "# Model Comparison\n",
        "models = ['TFT', 'N-HITS']\n",
        "accuracies = [tft_results['direction_accuracy'], nhits_results['direction_accuracy']]\n",
        "colors = ['skyblue', 'lightgreen']\n",
        "\n",
        "bars = axes[0,2].bar(models, accuracies, color=colors, alpha=0.8, edgecolor='black')\n",
        "axes[0,2].set_title('üéØ Direction Accuracy Comparison', fontweight='bold')\n",
        "axes[0,2].set_ylabel('Accuracy')\n",
        "axes[0,2].set_ylim(0, 1)\n",
        "axes[0,2].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add percentage labels on bars\n",
        "for bar, acc in zip(bars, accuracies):\n",
        "    height = bar.get_height()\n",
        "    axes[0,2].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                   f'{acc:.1%}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# TFT Predictions vs Actual\n",
        "sample_size = min(500, len(y_val))  # Limit points for clarity\n",
        "sample_indices = np.random.choice(len(y_val), sample_size, replace=False)\n",
        "\n",
        "axes[1,0].scatter(y_val[sample_indices], tft_predictions[sample_indices], \n",
        "                  alpha=0.6, color='blue', s=30)\n",
        "axes[1,0].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], \n",
        "               'r--', linewidth=2, label='Perfect Prediction')\n",
        "axes[1,0].set_title('üß† TFT: Predicted vs Actual', fontweight='bold')\n",
        "axes[1,0].set_xlabel('Actual Price Change')\n",
        "axes[1,0].set_ylabel('Predicted Price Change')\n",
        "axes[1,0].legend()\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "# N-HITS Predictions vs Actual\n",
        "axes[1,1].scatter(y_val[sample_indices], nhits_predictions[sample_indices], \n",
        "                  alpha=0.6, color='green', s=30)\n",
        "axes[1,1].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], \n",
        "               'r--', linewidth=2, label='Perfect Prediction')\n",
        "axes[1,1].set_title('üîÑ N-HITS: Predicted vs Actual', fontweight='bold')\n",
        "axes[1,1].set_xlabel('Actual Price Change')\n",
        "axes[1,1].set_ylabel('Predicted Price Change')\n",
        "axes[1,1].legend()\n",
        "axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "# Prediction Distribution\n",
        "axes[1,2].hist(y_val, bins=50, alpha=0.7, label='Actual', color='gray', density=True)\n",
        "axes[1,2].hist(tft_predictions, bins=50, alpha=0.7, label='TFT Predicted', color='blue', density=True)\n",
        "axes[1,2].hist(nhits_predictions, bins=50, alpha=0.7, label='N-HITS Predicted', color='green', density=True)\n",
        "axes[1,2].set_title('üìä Prediction Distributions', fontweight='bold')\n",
        "axes[1,2].set_xlabel('Price Change')\n",
        "axes[1,2].set_ylabel('Density')\n",
        "axes[1,2].legend()\n",
        "axes[1,2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ Training visualization completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deployment_header"
      },
      "source": [
        "## üì¶ 10. Cloudflare Workers Deployment Package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_deployment_package"
      },
      "outputs": [],
      "source": [
        "def extract_model_weights(model, model_name):\n",
        "    \"\"\"Extract model weights in Cloudflare Workers compatible format\"\"\"\n",
        "    \n",
        "    print(f\"üîß Extracting weights for {model_name}...\")\n",
        "    \n",
        "    weights_data = {\n",
        "        'model_name': model_name,\n",
        "        'architecture': {\n",
        "            'input_shape': list(model.input_shape),\n",
        "            'output_shape': list(model.output_shape),\n",
        "            'total_params': int(model.count_params()),\n",
        "            'sequence_length': SEQUENCE_LENGTH,\n",
        "            'num_features': NUM_FEATURES\n",
        "        },\n",
        "        'layers': [],\n",
        "        'normalization': {\n",
        "            'scalers': {symbol: {\n",
        "                'data_min': scaler.data_min_.tolist(),\n",
        "                'data_max': scaler.data_max_.tolist(),\n",
        "                'scale': scaler.scale_.tolist()\n",
        "            } for symbol, scaler in symbol_scalers.items()}\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Extract layer weights\n",
        "    for i, layer in enumerate(model.layers):\n",
        "        layer_weights = layer.get_weights()\n",
        "        \n",
        "        if len(layer_weights) > 0:  # Only include layers with weights\n",
        "            layer_info = {\n",
        "                'index': i,\n",
        "                'name': layer.name,\n",
        "                'type': layer.__class__.__name__,\n",
        "                'config': layer.get_config(),\n",
        "                'weights': [],\n",
        "                'weight_shapes': []\n",
        "            }\n",
        "            \n",
        "            # Convert weights to lists for JSON serialization\n",
        "            for weight in layer_weights:\n",
        "                layer_info['weights'].append(weight.flatten().tolist())\n",
        "                layer_info['weight_shapes'].append(list(weight.shape))\n",
        "            \n",
        "            weights_data['layers'].append(layer_info)\n",
        "            print(f\"   ‚úÖ {layer.name}: {len(layer_weights)} weight arrays\")\n",
        "    \n",
        "    return weights_data\n",
        "\n",
        "def create_deployment_metadata():\n",
        "    \"\"\"Create comprehensive deployment metadata\"\"\"\n",
        "    \n",
        "    metadata = {\n",
        "        'training_info': {\n",
        "            'date': datetime.now().isoformat(),\n",
        "            'symbols': SYMBOLS,\n",
        "            'sequence_length': SEQUENCE_LENGTH,\n",
        "            'num_features': NUM_FEATURES,\n",
        "            'training_samples': len(X_train),\n",
        "            'validation_samples': len(X_val),\n",
        "            'epochs_trained': {\n",
        "                'tft': len(tft_history.history['loss']),\n",
        "                'nhits': len(nhits_history.history['loss'])\n",
        "            },\n",
        "            'data_period': DATA_PERIOD,\n",
        "            'tensorflow_version': tf.__version__\n",
        "        },\n",
        "        'model_performance': {\n",
        "            'tft': {\n",
        "                'parameters': int(tft_model.count_params()),\n",
        "                'final_loss': float(min(tft_history.history['val_loss'])),\n",
        "                'final_mae': float(min(tft_history.history['val_mae'])),\n",
        "                'direction_accuracy': float(tft_results['direction_accuracy']),\n",
        "                'strong_prediction_accuracy': float(tft_results['strong_prediction_accuracy']),\n",
        "                'correlation': float(tft_results['correlation']),\n",
        "                'architecture': 'Temporal Fusion Transformer'\n",
        "            },\n",
        "            'nhits': {\n",
        "                'parameters': int(nhits_model.count_params()),\n",
        "                'final_loss': float(min(nhits_history.history['val_loss'])),\n",
        "                'final_mae': float(min(nhits_history.history['val_mae'])),\n",
        "                'direction_accuracy': float(nhits_results['direction_accuracy']),\n",
        "                'strong_prediction_accuracy': float(nhits_results['strong_prediction_accuracy']),\n",
        "                'correlation': float(nhits_results['correlation']),\n",
        "                'architecture': 'Neural Hierarchical Interpolation'\n",
        "            }\n",
        "        },\n",
        "        'deployment': {\n",
        "            'cloudflare_compatible': True,\n",
        "            'inference_type': 'genuine_neural_network',\n",
        "            'recommended_ensemble_weights': {\n",
        "                'tft': 0.55,\n",
        "                'nhits': 0.45\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    return metadata\n",
        "\n",
        "def create_deployment_guide():\n",
        "    \"\"\"Create comprehensive deployment guide\"\"\"\n",
        "    \n",
        "    guide = f\"\"\"# üöÄ Neural Network Deployment Guide\n",
        "\n",
        "## Overview\n",
        "This package contains trained TFT and N-HITS neural networks for financial prediction.\n",
        "\n",
        "## Model Performance\n",
        "- **TFT Model**: {tft_results['direction_accuracy']:.1%} direction accuracy ({tft_model.count_params():,} parameters)\n",
        "- **N-HITS Model**: {nhits_results['direction_accuracy']:.1%} direction accuracy ({nhits_model.count_params():,} parameters)\n",
        "- **Training Data**: {DATA_PERIOD} of market data ({len(X_train):,} training samples)\n",
        "- **Validation**: {len(X_val):,} samples\n",
        "\n",
        "## Files Included\n",
        "- `deployment_metadata.json`: Complete training and performance metadata\n",
        "- `tft_weights.json`: TFT model weights and architecture\n",
        "- `nhits_weights.json`: N-HITS model weights and architecture\n",
        "- `cloudflare_inference.js`: JavaScript inference implementation\n",
        "- `DEPLOYMENT_GUIDE.md`: This guide\n",
        "\n",
        "## Cloudflare Workers Deployment\n",
        "\n",
        "### 1. Upload to R2 Storage\n",
        "```bash\n",
        "wrangler r2 object put tft-trading-models/deployment_metadata.json --file=deployment_metadata.json\n",
        "wrangler r2 object put tft-trading-models/tft_weights.json --file=tft_weights.json\n",
        "wrangler r2 object put tft-trading-models/nhits_weights.json --file=nhits_weights.json\n",
        "```\n",
        "\n",
        "### 2. Update Worker Code\n",
        "Replace your existing model inference code with the generated JavaScript implementation.\n",
        "\n",
        "### 3. Deploy\n",
        "```bash\n",
        "wrangler deploy\n",
        "```\n",
        "\n",
        "### 4. Verify Deployment\n",
        "```bash\n",
        "curl -X POST \"https://your-worker.workers.dev/analyze\" \\\\\n",
        "     -H \"Content-Type: application/json\" \\\\\n",
        "     -d '{{\"symbols\": [\"AAPL\"], \"test_mode\": true}}'\n",
        "```\n",
        "\n",
        "Look for `inference_type: 'genuine_neural_network'` in the response.\n",
        "\n",
        "## Model Ensemble\n",
        "For best results, use ensemble prediction:\n",
        "- TFT weight: 55%\n",
        "- N-HITS weight: 45%\n",
        "\n",
        "## Technical Details\n",
        "- **Input**: 30-day sequences of OHLCV + VWAP data\n",
        "- **Output**: Next-day price change percentage\n",
        "- **Normalization**: MinMaxScaler per symbol\n",
        "- **Framework**: TensorFlow {tf.__version__}\n",
        "- **Training Date**: {datetime.now().strftime('%Y-%m-%d %H:%M UTC')}\n",
        "\n",
        "## Support\n",
        "Generated by Enhanced TFT + N-HITS Training Pipeline\n",
        "\"\"\"\n",
        "    \n",
        "    return guide\n",
        "\n",
        "# Extract weights from both models\n",
        "print(\"üîß Extracting model weights for deployment...\")\n",
        "tft_weights = extract_model_weights(tft_model, 'TFT')\n",
        "nhits_weights = extract_model_weights(nhits_model, 'NHITS')\n",
        "\n",
        "# Create deployment files\n",
        "print(\"\\nüì¶ Creating deployment package...\")\n",
        "\n",
        "deployment_files = {\n",
        "    'deployment_metadata.json': json.dumps(create_deployment_metadata(), indent=2),\n",
        "    'tft_weights.json': json.dumps(tft_weights, indent=2),\n",
        "    'nhits_weights.json': json.dumps(nhits_weights, indent=2),\n",
        "    'DEPLOYMENT_GUIDE.md': create_deployment_guide()\n",
        "}\n",
        "\n",
        "# Save files and create ZIP\n",
        "total_size = 0\n",
        "for filename, content in deployment_files.items():\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write(content)\n",
        "    \n",
        "    file_size = os.path.getsize(filename) / 1024  # KB\n",
        "    total_size += file_size\n",
        "    print(f\"   ‚úÖ Created {filename} ({file_size:.1f} KB)\")\n",
        "\n",
        "# Create ZIP package\n",
        "zip_filename = f\"enhanced_neural_networks_{datetime.now().strftime('%Y%m%d_%H%M')}.zip\"\n",
        "with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for filename in deployment_files.keys():\n",
        "        zipf.write(filename)\n",
        "        print(f\"   üì¶ Added {filename} to ZIP\")\n",
        "\n",
        "zip_size = os.path.getsize(zip_filename) / 1024 / 1024  # MB\n",
        "print(f\"\\nüì¶ Created {zip_filename} ({zip_size:.2f} MB)\")\n",
        "print(f\"üìä Total files size: {total_size:.1f} KB\")\n",
        "\n",
        "print(f\"\\n‚úÖ Deployment package creation completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download_header"
      },
      "source": [
        "## ‚¨áÔ∏è 11. Download Deployment Package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_package"
      },
      "outputs": [],
      "source": [
        "# Download the deployment package\n",
        "if IN_COLAB:\n",
        "    print(\"üîÑ Initiating download in Google Colab...\")\n",
        "    \n",
        "    # List all created files\n",
        "    deployment_files_list = [\n",
        "        'deployment_metadata.json',\n",
        "        'tft_weights.json', \n",
        "        'nhits_weights.json',\n",
        "        'DEPLOYMENT_GUIDE.md'\n",
        "    ]\n",
        "    \n",
        "    # Add the ZIP file\n",
        "    zip_files = [f for f in os.listdir('.') if f.startswith('enhanced_neural_networks_') and f.endswith('.zip')]\n",
        "    if zip_files:\n",
        "        main_zip = zip_files[0]\n",
        "        deployment_files_list.append(main_zip)\n",
        "    \n",
        "    print(\"\\nüìÅ Files ready for download:\")\n",
        "    for filename in deployment_files_list:\n",
        "        if os.path.exists(filename):\n",
        "            file_size = os.path.getsize(filename) / 1024 / 1024  # MB\n",
        "            print(f\"   ‚úÖ {filename} ({file_size:.2f} MB)\")\n",
        "        else:\n",
        "            print(f\"   ‚ùå {filename} (missing)\")\n",
        "    \n",
        "    # Download the main ZIP file\n",
        "    if zip_files:\n",
        "        print(f\"\\nüì¶ Downloading {main_zip}...\")\n",
        "        try:\n",
        "            files.download(main_zip)\n",
        "            print(\"‚úÖ Download initiated successfully!\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Download failed: {e}\")\n",
        "            print(\"üí° Try right-clicking the file in the file browser to download manually\")\n",
        "    \n",
        "    # Option to download individual files\n",
        "    print(\"\\n‚ùì Download individual files? (Run next cell if needed)\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è Not in Google Colab environment.\")\n",
        "    print(\"üìÅ Files saved locally:\")\n",
        "    \n",
        "    for filename in os.listdir('.'):\n",
        "        if filename.endswith(('.json', '.md', '.zip')) and ('enhanced_neural_networks' in filename or 'deployment' in filename or 'weights' in filename):\n",
        "            file_size = os.path.getsize(filename) / 1024  # KB\n",
        "            print(f\"   üìÑ {filename} ({file_size:.1f} KB)\")\n",
        "\n",
        "print(\"\\nüéâ Neural network training and deployment package creation completed!\")\n",
        "print(\"\\nüìä Summary:\")\n",
        "print(f\"   üß† TFT Model: {tft_results['direction_accuracy']:.1%} accuracy, {tft_model.count_params():,} parameters\")\n",
        "print(f\"   üîÑ N-HITS Model: {nhits_results['direction_accuracy']:.1%} accuracy, {nhits_model.count_params():,} parameters\")\n",
        "print(f\"   üìà Training Data: {len(X_train):,} samples over {DATA_PERIOD}\")\n",
        "print(f\"   üéØ Ready for Cloudflare Workers deployment!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_individual_files"
      },
      "outputs": [],
      "source": [
        "# Optional: Download individual files (run this cell if you want individual downloads)\n",
        "if IN_COLAB:\n",
        "    print(\"üì• Downloading individual files...\")\n",
        "    \n",
        "    files_to_download = [\n",
        "        'deployment_metadata.json',\n",
        "        'tft_weights.json',\n",
        "        'nhits_weights.json', \n",
        "        'DEPLOYMENT_GUIDE.md'\n",
        "    ]\n",
        "    \n",
        "    for filename in files_to_download:\n",
        "        if os.path.exists(filename):\n",
        "            try:\n",
        "                print(f\"   üìÑ Downloading {filename}...\")\n",
        "                files.download(filename)\n",
        "            except Exception as e:\n",
        "                print(f\"   ‚ùå Failed to download {filename}: {e}\")\n",
        "        else:\n",
        "            print(f\"   ‚ö†Ô∏è {filename} not found\")\n",
        "    \n",
        "    print(\"\\n‚úÖ Individual file downloads completed!\")\nelse:\n",
        "    print(\"‚ÑπÔ∏è Individual download only available in Google Colab\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine": "T4",
      "gpu": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}