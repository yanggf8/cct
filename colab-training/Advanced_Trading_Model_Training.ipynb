{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "advanced-trading-header"
      },
      "source": [
        "# 📈 Advanced Trading Model Training - Classification Approach\n",
        "\n",
        "**Based on Gemini's Recommendations:**\n",
        "- ✅ Feature engineering with technical indicators\n",
        "- ✅ Classification problem (UP/DOWN/HOLD)\n",
        "- ✅ Tree-based models (XGBoost/LightGBM)\n",
        "- ✅ Direction-focused training\n",
        "\n",
        "**Goal:** Achieve >65% direction accuracy for profitable trading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-dependencies"
      },
      "outputs": [],
      "source": [
        "# Install advanced ML packages\n",
        "!pip install yfinance pandas numpy scikit-learn\n",
        "!pip install xgboost lightgbm catboost\n",
        "!pip install ta-lib pandas-ta\n",
        "!pip install matplotlib seaborn plotly\n",
        "\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas_ta as ta\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import catboost as cb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"✅ Advanced ML packages installed successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data-collection-advanced"
      },
      "source": [
        "## 📊 Enhanced Data Collection with Market Context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fetch-enhanced-data"
      },
      "outputs": [],
      "source": [
        "# Enhanced symbols with market context\n",
        "SYMBOLS = ['AAPL', 'MSFT', 'GOOGL', 'TSLA', 'NVDA']\n",
        "MARKET_INDICES = ['^GSPC', '^IXIC', '^DJI']  # S&P 500, NASDAQ, Dow Jones\n",
        "VOLATILITY_INDEX = ['^VIX']  # Fear & Greed indicator\n",
        "\n",
        "def fetch_comprehensive_data(symbols, period=\"2y\"):\n",
        "    \"\"\"Fetch enhanced market data with context\"\"\"\n",
        "    all_data = {}\n",
        "    \n",
        "    # Fetch stock data\n",
        "    for symbol in symbols:\n",
        "        print(f\"📈 Fetching {symbol}...\")\n",
        "        ticker = yf.Ticker(symbol)\n",
        "        data = ticker.history(period=period)\n",
        "        \n",
        "        if len(data) > 0:\n",
        "            all_data[symbol] = data\n",
        "            print(f\"   ✅ {len(data)} data points\")\n",
        "    \n",
        "    # Fetch market indices for context\n",
        "    market_data = {}\n",
        "    for index in MARKET_INDICES + VOLATILITY_INDEX:\n",
        "        print(f\"📊 Fetching {index}...\")\n",
        "        ticker = yf.Ticker(index)\n",
        "        data = ticker.history(period=period)\n",
        "        if len(data) > 0:\n",
        "            market_data[index] = data\n",
        "            print(f\"   ✅ {len(data)} data points\")\n",
        "    \n",
        "    return all_data, market_data\n",
        "\n",
        "# Fetch comprehensive dataset\n",
        "print(\"🔄 Fetching comprehensive market data...\")\n",
        "stock_data, market_data = fetch_comprehensive_data(SYMBOLS)\n",
        "print(f\"\\n📊 Collected data for {len(stock_data)} stocks and {len(market_data)} market indices\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "advanced-feature-engineering"
      },
      "source": [
        "## 🔧 Advanced Feature Engineering - Technical Indicators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create-technical-features"
      },
      "outputs": [],
      "source": [
        "def create_technical_features(df, symbol_name):\n",
        "    \"\"\"Create comprehensive technical indicators\"\"\"\n",
        "    data = df.copy()\n",
        "    \n",
        "    print(f\"🔧 Creating technical features for {symbol_name}...\")\n",
        "    \n",
        "    # === TREND INDICATORS ===\n",
        "    # Moving Averages\n",
        "    data['SMA_5'] = ta.sma(data['Close'], length=5)\n",
        "    data['SMA_10'] = ta.sma(data['Close'], length=10)\n",
        "    data['SMA_20'] = ta.sma(data['Close'], length=20)\n",
        "    data['SMA_50'] = ta.sma(data['Close'], length=50)\n",
        "    data['EMA_12'] = ta.ema(data['Close'], length=12)\n",
        "    data['EMA_26'] = ta.ema(data['Close'], length=26)\n",
        "    \n",
        "    # MACD\n",
        "    macd = ta.macd(data['Close'])\n",
        "    data['MACD'] = macd['MACD_12_26_9']\n",
        "    data['MACD_Signal'] = macd['MACDs_12_26_9']\n",
        "    data['MACD_Histogram'] = macd['MACDh_12_26_9']\n",
        "    \n",
        "    # === MOMENTUM INDICATORS ===\n",
        "    # RSI\n",
        "    data['RSI_14'] = ta.rsi(data['Close'], length=14)\n",
        "    data['RSI_30'] = ta.rsi(data['Close'], length=30)\n",
        "    \n",
        "    # Stochastic\n",
        "    stoch = ta.stoch(data['High'], data['Low'], data['Close'])\n",
        "    data['Stoch_K'] = stoch['STOCHk_14_3_3']\n",
        "    data['Stoch_D'] = stoch['STOCHd_14_3_3']\n",
        "    \n",
        "    # Williams %R\n",
        "    data['Williams_R'] = ta.willr(data['High'], data['Low'], data['Close'])\n",
        "    \n",
        "    # === VOLATILITY INDICATORS ===\n",
        "    # Bollinger Bands\n",
        "    bb = ta.bbands(data['Close'])\n",
        "    data['BB_Upper'] = bb['BBU_20_2.0']\n",
        "    data['BB_Middle'] = bb['BBM_20_2.0']\n",
        "    data['BB_Lower'] = bb['BBL_20_2.0']\n",
        "    data['BB_Width'] = (data['BB_Upper'] - data['BB_Lower']) / data['BB_Middle']\n",
        "    data['BB_Position'] = (data['Close'] - data['BB_Lower']) / (data['BB_Upper'] - data['BB_Lower'])\n",
        "    \n",
        "    # Average True Range\n",
        "    data['ATR'] = ta.atr(data['High'], data['Low'], data['Close'])\n",
        "    \n",
        "    # === VOLUME INDICATORS ===\n",
        "    # Volume Moving Average\n",
        "    data['Volume_SMA'] = ta.sma(data['Volume'], length=20)\n",
        "    data['Volume_Ratio'] = data['Volume'] / data['Volume_SMA']\n",
        "    \n",
        "    # On Balance Volume\n",
        "    data['OBV'] = ta.obv(data['Close'], data['Volume'])\n",
        "    \n",
        "    # === PRICE ACTION FEATURES ===\n",
        "    # Returns\n",
        "    data['Return_1d'] = data['Close'].pct_change(1)\n",
        "    data['Return_3d'] = data['Close'].pct_change(3)\n",
        "    data['Return_5d'] = data['Close'].pct_change(5)\n",
        "    data['Return_10d'] = data['Close'].pct_change(10)\n",
        "    \n",
        "    # Price position within daily range\n",
        "    data['Price_Position'] = (data['Close'] - data['Low']) / (data['High'] - data['Low'])\n",
        "    \n",
        "    # Gap analysis\n",
        "    data['Gap'] = (data['Open'] - data['Close'].shift(1)) / data['Close'].shift(1)\n",
        "    \n",
        "    # === RELATIVE STRENGTH FEATURES ===\n",
        "    # Price relative to moving averages\n",
        "    data['Price_vs_SMA20'] = data['Close'] / data['SMA_20'] - 1\n",
        "    data['Price_vs_SMA50'] = data['Close'] / data['SMA_50'] - 1\n",
        "    \n",
        "    # Moving average slopes\n",
        "    data['SMA20_Slope'] = data['SMA_20'].pct_change(5)\n",
        "    data['SMA50_Slope'] = data['SMA_50'].pct_change(10)\n",
        "    \n",
        "    print(f\"   ✅ Created {len([col for col in data.columns if col not in ['Open', 'High', 'Low', 'Close', 'Volume']])} technical features\")\n",
        "    \n",
        "    return data\n",
        "\n",
        "def add_market_context(stock_df, market_data, symbol_name):\n",
        "    \"\"\"Add market context features\"\"\"\n",
        "    data = stock_df.copy()\n",
        "    \n",
        "    print(f\"🌐 Adding market context for {symbol_name}...\")\n",
        "    \n",
        "    # S&P 500 context\n",
        "    if '^GSPC' in market_data:\n",
        "        sp500 = market_data['^GSPC']['Close'].reindex(data.index, method='ffill')\n",
        "        data['SP500_Return'] = sp500.pct_change(1)\n",
        "        data['Beta_5d'] = data['Return_1d'].rolling(5).corr(data['SP500_Return'])\n",
        "        data['Relative_Strength'] = (data['Return_1d'] - data['SP500_Return']).rolling(20).mean()\n",
        "    \n",
        "    # VIX context (fear & greed)\n",
        "    if '^VIX' in market_data:\n",
        "        vix = market_data['^VIX']['Close'].reindex(data.index, method='ffill')\n",
        "        data['VIX'] = vix\n",
        "        data['VIX_Change'] = vix.pct_change(1)\n",
        "        data['Market_Fear'] = (vix > vix.rolling(20).mean()).astype(int)\n",
        "    \n",
        "    print(f\"   ✅ Added market context features\")\n",
        "    \n",
        "    return data\n",
        "\n",
        "# Create enhanced features for all stocks\n",
        "enhanced_data = {}\n",
        "for symbol in SYMBOLS:\n",
        "    if symbol in stock_data:\n",
        "        # Create technical features\n",
        "        enhanced = create_technical_features(stock_data[symbol], symbol)\n",
        "        \n",
        "        # Add market context\n",
        "        enhanced = add_market_context(enhanced, market_data, symbol)\n",
        "        \n",
        "        # Add symbol identifier\n",
        "        enhanced['Symbol'] = symbol\n",
        "        \n",
        "        enhanced_data[symbol] = enhanced\n",
        "\n",
        "print(f\"\\n✅ Enhanced feature engineering completed for {len(enhanced_data)} symbols\")\n",
        "print(f\"📊 Total features per symbol: {len(enhanced_data[SYMBOLS[0]].columns)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "classification-target"
      },
      "source": [
        "## 🎯 Classification Target Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create-classification-targets"
      },
      "outputs": [],
      "source": [
        "def create_classification_targets(data, prediction_days=1, threshold=0.01):\n",
        "    \"\"\"Create classification targets (UP/DOWN/HOLD)\"\"\"\n",
        "    \n",
        "    # Calculate future returns\n",
        "    future_return = data['Close'].shift(-prediction_days) / data['Close'] - 1\n",
        "    \n",
        "    # Create classification labels\n",
        "    conditions = [\n",
        "        future_return > threshold,    # UP\n",
        "        future_return < -threshold,   # DOWN\n",
        "    ]\n",
        "    choices = [1, -1]  # UP=1, DOWN=-1, HOLD=0 (default)\n",
        "    \n",
        "    labels = np.select(conditions, choices, default=0)\n",
        "    \n",
        "    return labels, future_return\n",
        "\n",
        "def prepare_classification_dataset(enhanced_data, prediction_days=1, threshold=0.01):\n",
        "    \"\"\"Prepare complete dataset for classification\"\"\"\n",
        "    \n",
        "    all_features = []\n",
        "    all_targets = []\n",
        "    all_returns = []\n",
        "    metadata = []\n",
        "    \n",
        "    print(f\"🎯 Creating classification targets (threshold: ±{threshold:.1%})...\")\n",
        "    \n",
        "    for symbol in SYMBOLS:\n",
        "        if symbol in enhanced_data:\n",
        "            data = enhanced_data[symbol].copy()\n",
        "            \n",
        "            # Create targets\n",
        "            targets, future_returns = create_classification_targets(\n",
        "                data, prediction_days, threshold\n",
        "            )\n",
        "            \n",
        "            # Remove non-feature columns\n",
        "            feature_cols = [col for col in data.columns \n",
        "                          if col not in ['Open', 'High', 'Low', 'Close', 'Volume', 'Symbol']]\n",
        "            \n",
        "            features = data[feature_cols].copy()\n",
        "            \n",
        "            # Remove rows with NaN values\n",
        "            valid_mask = ~(features.isna().any(axis=1) | np.isnan(targets))\n",
        "            \n",
        "            features_clean = features[valid_mask]\n",
        "            targets_clean = targets[valid_mask]\n",
        "            returns_clean = future_returns[valid_mask]\n",
        "            \n",
        "            # Store data\n",
        "            all_features.append(features_clean)\n",
        "            all_targets.extend(targets_clean)\n",
        "            all_returns.extend(returns_clean)\n",
        "            \n",
        "            # Metadata\n",
        "            for i in range(len(features_clean)):\n",
        "                metadata.append({\n",
        "                    'symbol': symbol,\n",
        "                    'date': features_clean.index[i],\n",
        "                    'target': targets_clean[i],\n",
        "                    'return': returns_clean[i]\n",
        "                })\n",
        "            \n",
        "            # Class distribution for this symbol\n",
        "            unique, counts = np.unique(targets_clean, return_counts=True)\n",
        "            class_dist = dict(zip(unique, counts))\n",
        "            total = sum(counts)\n",
        "            \n",
        "            print(f\"   📊 {symbol}:\")\n",
        "            print(f\"      DOWN (-1): {class_dist.get(-1, 0):3d} ({class_dist.get(-1, 0)/total:.1%})\")\n",
        "            print(f\"      HOLD ( 0): {class_dist.get(0, 0):3d} ({class_dist.get(0, 0)/total:.1%})\")\n",
        "            print(f\"      UP   ( 1): {class_dist.get(1, 0):3d} ({class_dist.get(1, 0)/total:.1%})\")\n",
        "    \n",
        "    # Combine all features\n",
        "    X = pd.concat(all_features, ignore_index=True)\n",
        "    y = np.array(all_targets)\n",
        "    returns = np.array(all_returns)\n",
        "    \n",
        "    print(f\"\\n📊 Complete dataset:\")\n",
        "    print(f\"   Features shape: {X.shape}\")\n",
        "    print(f\"   Targets shape: {y.shape}\")\n",
        "    \n",
        "    # Overall class distribution\n",
        "    unique, counts = np.unique(y, return_counts=True)\n",
        "    total = len(y)\n",
        "    print(f\"\\n🎯 Overall class distribution:\")\n",
        "    for label, count in zip(unique, counts):\n",
        "        label_name = {-1: 'DOWN', 0: 'HOLD', 1: 'UP'}[label]\n",
        "        print(f\"   {label_name}: {count:4d} ({count/total:.1%})\")\n",
        "    \n",
        "    return X, y, returns, metadata\n",
        "\n",
        "# Create classification dataset\n",
        "X, y, future_returns, metadata = prepare_classification_dataset(\n",
        "    enhanced_data, \n",
        "    prediction_days=1, \n",
        "    threshold=0.005  # 0.5% threshold for UP/DOWN\n",
        ")\n",
        "\n",
        "print(f\"\\n✅ Classification dataset prepared with {len(X)} samples and {len(X.columns)} features\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model-training-advanced"
      },
      "source": [
        "## 🏋️ Advanced Model Training - Tree-Based Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train-classification-models"
      },
      "outputs": [],
      "source": [
        "# Prepare train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"📊 Data splits:\")\n",
        "print(f\"   Training: {len(X_train)} samples\")\n",
        "print(f\"   Testing: {len(X_test)} samples\")\n",
        "\n",
        "# Scale features for some models\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "models_to_train = {\n",
        "    'RandomForest': RandomForestClassifier(\n",
        "        n_estimators=200,\n",
        "        max_depth=15,\n",
        "        min_samples_split=10,\n",
        "        min_samples_leaf=5,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ),\n",
        "    'XGBoost': xgb.XGBClassifier(\n",
        "        n_estimators=200,\n",
        "        max_depth=8,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ),\n",
        "    'LightGBM': lgb.LGBMClassifier(\n",
        "        n_estimators=200,\n",
        "        max_depth=8,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        verbose=-1\n",
        "    )\n",
        "}\n",
        "\n",
        "trained_models = {}\n",
        "model_results = {}\n",
        "\n",
        "print(\"\\n🏋️ Training advanced classification models...\\n\")\n",
        "\n",
        "for name, model in models_to_train.items():\n",
        "    print(f\"🔄 Training {name}...\")\n",
        "    \n",
        "    # Train model\n",
        "    if name in ['RandomForest', 'XGBoost', 'LightGBM']:  # Tree-based models\n",
        "        model.fit(X_train, y_train)\n",
        "        predictions = model.predict(X_test)\n",
        "        probabilities = model.predict_proba(X_test)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    \n",
        "    # Direction accuracy (most important for trading)\n",
        "    direction_mask = y_test != 0  # Exclude HOLD predictions\n",
        "    if np.sum(direction_mask) > 0:\n",
        "        direction_accuracy = accuracy_score(\n",
        "            y_test[direction_mask], \n",
        "            predictions[direction_mask]\n",
        "        )\n",
        "    else:\n",
        "        direction_accuracy = 0\n",
        "    \n",
        "    # Store results\n",
        "    model_results[name] = {\n",
        "        'model': model,\n",
        "        'accuracy': accuracy,\n",
        "        'direction_accuracy': direction_accuracy,\n",
        "        'predictions': predictions,\n",
        "        'probabilities': probabilities\n",
        "    }\n",
        "    \n",
        "    trained_models[name] = model\n",
        "    \n",
        "    print(f\"   ✅ {name}:\")\n",
        "    print(f\"      Overall Accuracy: {accuracy:.1%}\")\n",
        "    print(f\"      Direction Accuracy: {direction_accuracy:.1%}\")\n",
        "    print(f\"      (Direction accuracy is most important for trading)\")\n",
        "    print()\n",
        "\n",
        "print(\"✅ All models trained successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model-evaluation-advanced"
      },
      "source": [
        "## 📊 Comprehensive Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evaluate-classification-models"
      },
      "outputs": [],
      "source": [
        "def evaluate_trading_performance(y_true, y_pred, returns, model_name):\n",
        "    \"\"\"Evaluate model from trading perspective\"\"\"\n",
        "    \n",
        "    # Basic classification metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    \n",
        "    # Direction-only accuracy (excluding HOLD)\n",
        "    direction_mask = (y_true != 0) & (y_pred != 0)\n",
        "    direction_accuracy = accuracy_score(y_true[direction_mask], y_pred[direction_mask]) if np.sum(direction_mask) > 0 else 0\n",
        "    \n",
        "    # Trading simulation\n",
        "    trading_returns = []\n",
        "    correct_predictions = 0\n",
        "    total_trades = 0\n",
        "    \n",
        "    for i in range(len(y_true)):\n",
        "        if y_pred[i] != 0:  # Only trade on UP/DOWN predictions\n",
        "            total_trades += 1\n",
        "            \n",
        "            # Simulate trading based on prediction\n",
        "            if y_pred[i] == 1:  # Predicted UP, go long\n",
        "                trade_return = returns[i]\n",
        "            else:  # Predicted DOWN, go short\n",
        "                trade_return = -returns[i]\n",
        "            \n",
        "            trading_returns.append(trade_return)\n",
        "            \n",
        "            # Check if prediction was correct\n",
        "            if (y_pred[i] > 0 and returns[i] > 0) or (y_pred[i] < 0 and returns[i] < 0):\n",
        "                correct_predictions += 1\n",
        "    \n",
        "    # Trading metrics\n",
        "    if len(trading_returns) > 0:\n",
        "        avg_return = np.mean(trading_returns)\n",
        "        total_return = np.sum(trading_returns)\n",
        "        win_rate = correct_predictions / total_trades if total_trades > 0 else 0\n",
        "        sharpe_ratio = np.mean(trading_returns) / np.std(trading_returns) if np.std(trading_returns) > 0 else 0\n",
        "    else:\n",
        "        avg_return = total_return = win_rate = sharpe_ratio = 0\n",
        "    \n",
        "    results = {\n",
        "        'model': model_name,\n",
        "        'overall_accuracy': accuracy,\n",
        "        'direction_accuracy': direction_accuracy,\n",
        "        'total_trades': total_trades,\n",
        "        'win_rate': win_rate,\n",
        "        'avg_return_per_trade': avg_return,\n",
        "        'total_return': total_return,\n",
        "        'sharpe_ratio': sharpe_ratio\n",
        "    }\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Evaluate all models\n",
        "print(\"📊 Comprehensive Model Evaluation\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "evaluation_results = []\n",
        "\n",
        "for name, result in model_results.items():\n",
        "    # Get test returns for this model's predictions\n",
        "    test_returns = future_returns[-len(y_test):]\n",
        "    \n",
        "    eval_result = evaluate_trading_performance(\n",
        "        y_test, \n",
        "        result['predictions'], \n",
        "        test_returns, \n",
        "        name\n",
        "    )\n",
        "    \n",
        "    evaluation_results.append(eval_result)\n",
        "    \n",
        "    print(f\"🤖 {name} Performance:\")\n",
        "    print(f\"   📈 Overall Accuracy: {eval_result['overall_accuracy']:.1%}\")\n",
        "    print(f\"   🎯 Direction Accuracy: {eval_result['direction_accuracy']:.1%} ⭐\")\n",
        "    print(f\"   💼 Total Trades: {eval_result['total_trades']}\")\n",
        "    print(f\"   🏆 Win Rate: {eval_result['win_rate']:.1%}\")\n",
        "    print(f\"   💰 Avg Return/Trade: {eval_result['avg_return_per_trade']:.2%}\")\n",
        "    print(f\"   📊 Total Return: {eval_result['total_return']:.2%}\")\n",
        "    print(f\"   📈 Sharpe Ratio: {eval_result['sharpe_ratio']:.2f}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# Find best model by direction accuracy\n",
        "best_model = max(evaluation_results, key=lambda x: x['direction_accuracy'])\n",
        "print(f\"\\n🏆 BEST MODEL: {best_model['model']} with {best_model['direction_accuracy']:.1%} direction accuracy\")\n",
        "\n",
        "# Check if any model beats 60% direction accuracy threshold\n",
        "profitable_models = [r for r in evaluation_results if r['direction_accuracy'] > 0.60]\n",
        "if profitable_models:\n",
        "    print(f\"\\n✅ PROFITABLE MODELS ({len(profitable_models)} models > 60% direction accuracy):\")\n",
        "    for model in profitable_models:\n",
        "        print(f\"   🎯 {model['model']}: {model['direction_accuracy']:.1%} direction accuracy\")\n",
        "else:\n",
        "    print(f\"\\n⚠️  NO MODELS EXCEED 60% DIRECTION ACCURACY THRESHOLD\")\n",
        "    print(f\"   Consider: More feature engineering, longer training periods, or different approaches\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feature-importance"
      },
      "source": [
        "## 🔍 Feature Importance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "analyze-feature-importance"
      },
      "outputs": [],
      "source": [
        "def analyze_feature_importance(model, feature_names, model_name, top_n=20):\n",
        "    \"\"\"Analyze and visualize feature importance\"\"\"\n",
        "    \n",
        "    if hasattr(model, 'feature_importances_'):\n",
        "        importances = model.feature_importances_\n",
        "        \n",
        "        # Create feature importance DataFrame\n",
        "        feature_imp = pd.DataFrame({\n",
        "            'feature': feature_names,\n",
        "            'importance': importances\n",
        "        }).sort_values('importance', ascending=False)\n",
        "        \n",
        "        print(f\"\\n🔍 {model_name} - Top {top_n} Most Important Features:\")\n",
        "        print(\"=\" * 60)\n",
        "        \n",
        "        for i, row in feature_imp.head(top_n).iterrows():\n",
        "            print(f\"{row['feature']:30s} {row['importance']:.4f}\")\n",
        "        \n",
        "        # Visualize top features\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        top_features = feature_imp.head(top_n)\n",
        "        \n",
        "        sns.barplot(data=top_features, y='feature', x='importance')\n",
        "        plt.title(f'{model_name} - Top {top_n} Feature Importances')\n",
        "        plt.xlabel('Importance')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        return feature_imp\n",
        "    else:\n",
        "        print(f\"⚠️ {model_name} does not support feature importance\")\n",
        "        return None\n",
        "\n",
        "# Analyze feature importance for tree-based models\n",
        "feature_importance_results = {}\n",
        "\n",
        "for name, model in trained_models.items():\n",
        "    if name in ['RandomForest', 'XGBoost', 'LightGBM']:\n",
        "        importance_df = analyze_feature_importance(\n",
        "            model, \n",
        "            X.columns.tolist(), \n",
        "            name, \n",
        "            top_n=15\n",
        "        )\n",
        "        feature_importance_results[name] = importance_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cloudflare-deployment-advanced"
      },
      "source": [
        "## 🚀 Cloudflare Workers Deployment Package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create-advanced-deployment"
      },
      "outputs": [],
      "source": [
        "def create_advanced_deployment_package(best_model_name, model, scaler, feature_names):\n",
        "    \"\"\"Create deployment package for the best performing model\"\"\"\n",
        "    \n",
        "    # Get model performance\n",
        "    best_result = next(r for r in evaluation_results if r['model'] == best_model_name)\n",
        "    \n",
        "    # Create metadata\n",
        "    metadata = {\n",
        "        'training_date': datetime.now().isoformat(),\n",
        "        'model_type': best_model_name,\n",
        "        'problem_type': 'classification',\n",
        "        'target_classes': {'-1': 'DOWN', '0': 'HOLD', '1': 'UP'},\n",
        "        'performance': {\n",
        "            'overall_accuracy': best_result['overall_accuracy'],\n",
        "            'direction_accuracy': best_result['direction_accuracy'],\n",
        "            'win_rate': best_result['win_rate'],\n",
        "            'total_trades': best_result['total_trades'],\n",
        "            'avg_return_per_trade': best_result['avg_return_per_trade'],\n",
        "            'sharpe_ratio': best_result['sharpe_ratio']\n",
        "        },\n",
        "        'features': {\n",
        "            'count': len(feature_names),\n",
        "            'names': feature_names,\n",
        "            'scaling_required': True\n",
        "        },\n",
        "        'trading_info': {\n",
        "            'prediction_threshold': 0.005,  # 0.5%\n",
        "            'symbols': SYMBOLS,\n",
        "            'recommendation': 'DEPLOY' if best_result['direction_accuracy'] > 0.60 else 'NEEDS_IMPROVEMENT'\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Create JavaScript inference code\n",
        "    js_inference = f'''\n",
        "/**\n",
        " * Advanced Trading Model - {best_model_name} Classification\n",
        " * Generated: {datetime.now().isoformat()}\n",
        " * Direction Accuracy: {best_result['direction_accuracy']:.1%}\n",
        " */\n",
        "\n",
        "// Model configuration\n",
        "const MODEL_CONFIG = {{\n",
        "    type: '{best_model_name}',\n",
        "    directionAccuracy: {best_result['direction_accuracy']:.3f},\n",
        "    features: {len(feature_names)},\n",
        "    threshold: 0.005\n",
        "}};\n",
        "\n",
        "// Feature names (must match training order)\n",
        "const FEATURE_NAMES = {feature_names};\n",
        "\n",
        "export async function runAdvancedPrediction(ohlcv, env) {{\n",
        "    try {{\n",
        "        // 1. Calculate technical indicators\n",
        "        const technicalFeatures = calculateTechnicalIndicators(ohlcv);\n",
        "        \n",
        "        // 2. Add market context\n",
        "        const marketFeatures = await addMarketContext(technicalFeatures, env);\n",
        "        \n",
        "        // 3. Scale features\n",
        "        const scaledFeatures = scaleFeatures(marketFeatures);\n",
        "        \n",
        "        // 4. Run model prediction\n",
        "        const prediction = run{best_model_name}Prediction(scaledFeatures);\n",
        "        \n",
        "        // 5. Interpret results\n",
        "        const result = {{\n",
        "            predicted_direction: prediction > 0 ? 'UP' : prediction < 0 ? 'DOWN' : 'HOLD',\n",
        "            confidence: Math.abs(prediction) / Math.max(Math.abs(prediction), 1),\n",
        "            model_type: '{best_model_name}',\n",
        "            direction_accuracy: {best_result['direction_accuracy']:.3f},\n",
        "            recommendation: prediction !== 0 ? 'TRADE' : 'HOLD'\n",
        "        }};\n",
        "        \n",
        "        return result;\n",
        "        \n",
        "    }} catch (error) {{\n",
        "        console.error('Advanced prediction error:', error);\n",
        "        return {{\n",
        "            predicted_direction: 'HOLD',\n",
        "            confidence: 0,\n",
        "            error: error.message\n",
        "        }};\n",
        "    }}\n",
        "}}\n",
        "\n",
        "function calculateTechnicalIndicators(ohlcv) {{\n",
        "    // Implementation for calculating all technical indicators\n",
        "    // This would include RSI, MACD, Bollinger Bands, etc.\n",
        "    \n",
        "    const features = {{}};\n",
        "    \n",
        "    // RSI calculation\n",
        "    features.RSI_14 = calculateRSI(ohlcv, 14);\n",
        "    \n",
        "    // MACD calculation\n",
        "    const macd = calculateMACD(ohlcv);\n",
        "    features.MACD = macd.macd;\n",
        "    features.MACD_Signal = macd.signal;\n",
        "    \n",
        "    // Add all other technical indicators...\n",
        "    \n",
        "    return features;\n",
        "}}\n",
        "\n",
        "// Note: Full implementation would include all technical indicator calculations\n",
        "// and the complete {best_model_name} prediction algorithm\n",
        "'''\n",
        "    \n",
        "    # Save files\n",
        "    with open('advanced_trading_metadata.json', 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "    \n",
        "    with open('advanced_trading_inference.js', 'w') as f:\n",
        "        f.write(js_inference)\n",
        "    \n",
        "    # Create deployment guide\n",
        "    deployment_guide = f'''\n",
        "# Advanced Trading Model Deployment Guide\n",
        "\n",
        "## Model Performance\n",
        "- **Model Type**: {best_model_name}\n",
        "- **Direction Accuracy**: {best_result['direction_accuracy']:.1%}\n",
        "- **Win Rate**: {best_result['win_rate']:.1%}\n",
        "- **Total Trades**: {best_result['total_trades']}\n",
        "- **Recommendation**: {metadata['trading_info']['recommendation']}\n",
        "\n",
        "## Key Improvements\n",
        "✅ Advanced feature engineering (technical indicators)\n",
        "✅ Classification approach (UP/DOWN/HOLD)\n",
        "✅ Tree-based models optimized for tabular data\n",
        "✅ Trading-focused evaluation metrics\n",
        "\n",
        "## Deployment Status\n",
        "{'🟢 READY FOR DEPLOYMENT' if best_result['direction_accuracy'] > 0.60 else '🟡 NEEDS FURTHER IMPROVEMENT'}\n",
        "\n",
        "Direction accuracy > 60% is generally required for profitable trading.\n",
        "Current model: {best_result['direction_accuracy']:.1%}\n",
        "'''\n",
        "    \n",
        "    with open('ADVANCED_DEPLOYMENT_GUIDE.md', 'w') as f:\n",
        "        f.write(deployment_guide)\n",
        "    \n",
        "    print(f\"📦 Advanced deployment package created!\")\n",
        "    print(f\"   📊 Model: {best_model_name}\")\n",
        "    print(f\"   🎯 Direction Accuracy: {best_result['direction_accuracy']:.1%}\")\n",
        "    print(f\"   📁 Files: metadata, inference code, deployment guide\")\n",
        "    \n",
        "    return metadata\n",
        "\n",
        "# Create deployment package for best model\n",
        "if evaluation_results:\n",
        "    best_model_name = best_model['model']\n",
        "    best_model_obj = trained_models[best_model_name]\n",
        "    \n",
        "    deployment_metadata = create_advanced_deployment_package(\n",
        "        best_model_name,\n",
        "        best_model_obj,\n",
        "        scaler,\n",
        "        X.columns.tolist()\n",
        "    )\n",
        "    \n",
        "    print(f\"\\n🎉 Advanced training completed!\")\n",
        "    print(f\"📈 Best model achieved {best_model['direction_accuracy']:.1%} direction accuracy\")\n",
        "    \n",
        "    if best_model['direction_accuracy'] > 0.60:\n",
        "        print(f\"✅ Model exceeds 60% threshold - READY FOR PRODUCTION\")\n",
        "    else:\n",
        "        print(f\"⚠️  Model below 60% threshold - Consider more feature engineering\")\n",
        "        print(f\"💡 Suggestions: More data, ensemble methods, or alternative approaches\")\n",
        "else:\n",
        "    print(\"❌ No models were successfully trained\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}