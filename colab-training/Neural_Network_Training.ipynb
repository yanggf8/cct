{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# Neural Network Training for Financial Predictions\n",
        "\n",
        "**Goal**: Train TFT and N-HITS models in Colab, export trained weights for Vercel deployment\n",
        "\n",
        "**Process**:\n",
        "1. Train models on real financial data in Colab (with GPU)\n",
        "2. Save trained weights in TensorFlow.js format\n",
        "3. Download weights to deploy on Vercel\n",
        "4. Update Vercel endpoints to load these trained weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install tensorflow yfinance pandas numpy scikit-learn tensorflowjs\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import json\n",
        "import os\n",
        "\n",
        "print(f\"TensorFlow: {tf.__version__}\")\n",
        "print(f\"GPU: {tf.config.list_physical_devices('GPU')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data"
      },
      "source": [
        "## 1. Data Collection and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fetch_data"
      },
      "outputs": [],
      "source": [
        "# Fetch real market data\n",
        "symbols = ['AAPL', 'MSFT', 'GOOGL', 'TSLA', 'NVDA']\n",
        "print(\"Fetching market data...\")\n",
        "\n",
        "all_data = []\n",
        "for symbol in symbols:\n",
        "    ticker = yf.Ticker(symbol)\n",
        "    data = ticker.history(period='2y', interval='1d')\n",
        "    data['Symbol'] = symbol\n",
        "    all_data.append(data)\n",
        "    print(f\"{symbol}: {len(data)} days\")\n",
        "\n",
        "combined_data = pd.concat(all_data)\n",
        "print(f\"Total: {len(combined_data)} data points\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "preprocess"
      },
      "outputs": [],
      "source": [
        "# Create sequences for training\n",
        "def create_sequences(data, seq_length=30):\n",
        "    X, y = [], []\n",
        "    \n",
        "    for symbol in data['Symbol'].unique():\n",
        "        symbol_data = data[data['Symbol'] == symbol].sort_index()\n",
        "        \n",
        "        # Features: OHLCV + technical indicators\n",
        "        features = symbol_data[['Open', 'High', 'Low', 'Close', 'Volume']].values\n",
        "        \n",
        "        # Add price changes\n",
        "        price_changes = np.diff(symbol_data['Close'].values, prepend=symbol_data['Close'].values[0])\n",
        "        price_changes = price_changes / symbol_data['Close'].values  # Percentage change\n",
        "        \n",
        "        # Combine features\n",
        "        enhanced_features = np.column_stack([features, price_changes])\n",
        "        \n",
        "        # Normalize\n",
        "        scaler = MinMaxScaler()\n",
        "        scaled_features = scaler.fit_transform(enhanced_features)\n",
        "        \n",
        "        # Create sequences\n",
        "        for i in range(seq_length, len(scaled_features)):\n",
        "            X.append(scaled_features[i-seq_length:i])  # Past 30 days\n",
        "            \n",
        "            # Target: next day price change\n",
        "            current_price = symbol_data.iloc[i-1]['Close']\n",
        "            next_price = symbol_data.iloc[i]['Close']\n",
        "            target = (next_price - current_price) / current_price\n",
        "            y.append(target)\n",
        "    \n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Create dataset\n",
        "X, y = create_sequences(combined_data)\n",
        "print(f\"Dataset shape: X={X.shape}, y={y.shape}\")\n",
        "\n",
        "# Train/test split\n",
        "split = int(0.8 * len(X))\n",
        "X_train, X_test = X[:split], X[split:]\n",
        "y_train, y_test = y[:split], y[split:]\n",
        "\n",
        "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "models"
      },
      "source": [
        "## 2. Model Architectures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tft_model"
      },
      "outputs": [],
      "source": [
        "# TFT Model (simplified for training)\n",
        "def create_tft_model(input_shape):\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "    \n",
        "    # Variable selection\n",
        "    context = tf.keras.layers.GlobalAveragePooling1D()(inputs)\n",
        "    selection_weights = tf.keras.layers.Dense(input_shape[-1], activation='softmax')(context)\n",
        "    selection_weights = tf.keras.layers.RepeatVector(input_shape[0])(selection_weights)\n",
        "    selected_features = tf.keras.layers.Multiply()([inputs, selection_weights])\n",
        "    \n",
        "    # LSTM processing\n",
        "    lstm_out = tf.keras.layers.LSTM(64, return_sequences=True, dropout=0.2)(selected_features)\n",
        "    \n",
        "    # Multi-head attention\n",
        "    attention = tf.keras.layers.MultiHeadAttention(num_heads=4, key_dim=16)(lstm_out, lstm_out)\n",
        "    attention = tf.keras.layers.LayerNormalization()(attention + lstm_out)\n",
        "    \n",
        "    # Output\n",
        "    pooled = tf.keras.layers.GlobalAveragePooling1D()(attention)\n",
        "    dense = tf.keras.layers.Dense(32, activation='relu')(pooled)\n",
        "    output = tf.keras.layers.Dense(1)(dense)\n",
        "    \n",
        "    model = tf.keras.Model(inputs, output, name='TFT')\n",
        "    return model\n",
        "\n",
        "tft_model = create_tft_model((X.shape[1], X.shape[2]))\n",
        "print(f\"TFT Model: {tft_model.count_params():,} parameters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhits_model"
      },
      "outputs": [],
      "source": [
        "# N-HITS Model (hierarchical blocks)\n",
        "def create_nhits_model(input_shape):\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "    \n",
        "    # Input projection\n",
        "    x = tf.keras.layers.Dense(128)(inputs)\n",
        "    \n",
        "    # Hierarchical stacks\n",
        "    stack_outputs = []\n",
        "    \n",
        "    for pool_size in [2, 4, 8]:  # Different time scales\n",
        "        # Downsample\n",
        "        downsampled = tf.keras.layers.AveragePooling1D(pool_size, padding='same')(x)\n",
        "        \n",
        "        # MLP blocks\n",
        "        mlp = tf.keras.layers.Dense(128, activation='relu')(downsampled)\n",
        "        mlp = tf.keras.layers.Dense(128, activation='relu')(mlp)\n",
        "        \n",
        "        # Upsample back\n",
        "        upsampled = tf.keras.layers.UpSampling1D(pool_size)(mlp)\n",
        "        \n",
        "        # Ensure same length as original\n",
        "        if upsampled.shape[1] != input_shape[0]:\n",
        "            upsampled = upsampled[:, :input_shape[0], :]\n",
        "        \n",
        "        stack_outputs.append(upsampled)\n",
        "    \n",
        "    # Combine hierarchical outputs\n",
        "    combined = tf.keras.layers.Add()(stack_outputs)\n",
        "    \n",
        "    # Final prediction\n",
        "    pooled = tf.keras.layers.GlobalAveragePooling1D()(combined)\n",
        "    dense = tf.keras.layers.Dense(64, activation='relu')(pooled)\n",
        "    output = tf.keras.layers.Dense(1)(dense)\n",
        "    \n",
        "    model = tf.keras.Model(inputs, output, name='NHITS')\n",
        "    return model\n",
        "\n",
        "nhits_model = create_nhits_model((X.shape[1], X.shape[2]))\n",
        "print(f\"N-HITS Model: {nhits_model.count_params():,} parameters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training"
      },
      "source": [
        "## 3. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_tft"
      },
      "outputs": [],
      "source": [
        "# Train TFT Model\n",
        "print(\"Training TFT...\")\n",
        "\n",
        "tft_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mse',\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "tft_history = tft_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    verbose=1,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(patience=3)\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"TFT Training Complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_nhits"
      },
      "outputs": [],
      "source": [
        "# Train N-HITS Model\n",
        "print(\"Training N-HITS...\")\n",
        "\n",
        "nhits_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mse',\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "nhits_history = nhits_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    verbose=1,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(patience=3)\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"N-HITS Training Complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evaluation"
      },
      "source": [
        "## 4. Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evaluate"
      },
      "outputs": [],
      "source": [
        "# Evaluate models\n",
        "tft_loss, tft_mae = tft_model.evaluate(X_test, y_test, verbose=0)\n",
        "nhits_loss, nhits_mae = nhits_model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"TFT - Loss: {tft_loss:.6f}, MAE: {tft_mae:.6f}\")\n",
        "print(f\"N-HITS - Loss: {nhits_loss:.6f}, MAE: {nhits_mae:.6f}\")\n",
        "\n",
        "# Direction accuracy\n",
        "tft_pred = tft_model.predict(X_test[:100], verbose=0)\n",
        "nhits_pred = nhits_model.predict(X_test[:100], verbose=0)\n",
        "actual = y_test[:100]\n",
        "\n",
        "tft_dir_acc = np.mean(np.sign(tft_pred.flatten()) == np.sign(actual))\n",
        "nhits_dir_acc = np.mean(np.sign(nhits_pred.flatten()) == np.sign(actual))\n",
        "\n",
        "print(f\"\\nDirection Accuracy:\")\n",
        "print(f\"TFT: {tft_dir_acc:.1%}\")\n",
        "print(f\"N-HITS: {nhits_dir_acc:.1%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "export"
      },
      "source": [
        "## 5. Export for Vercel Deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "export_models"
      },
      "outputs": [],
      "source": [
        "import tensorflowjs as tfjs\n",
        "import zipfile\n",
        "\n",
        "# Create export directory\n",
        "os.makedirs('/content/models', exist_ok=True)\n",
        "\n",
        "print(\"Exporting trained models...\")\n",
        "\n",
        "# Convert to TensorFlow.js format\n",
        "tfjs.converters.save_keras_model(tft_model, '/content/models/tft-trained')\n",
        "tfjs.converters.save_keras_model(nhits_model, '/content/models/nhits-trained')\n",
        "\n",
        "print(\"Models exported to TensorFlow.js format\")\n",
        "\n",
        "# Save model metadata\n",
        "metadata = {\n",
        "    'tft': {\n",
        "        'loss': float(tft_loss),\n",
        "        'mae': float(tft_mae),\n",
        "        'direction_accuracy': float(tft_dir_acc),\n",
        "        'parameters': int(tft_model.count_params())\n",
        "    },\n",
        "    'nhits': {\n",
        "        'loss': float(nhits_loss),\n",
        "        'mae': float(nhits_mae),\n",
        "        'direction_accuracy': float(nhits_dir_acc),\n",
        "        'parameters': int(nhits_model.count_params())\n",
        "    },\n",
        "    'training_info': {\n",
        "        'sequence_length': int(X.shape[1]),\n",
        "        'num_features': int(X.shape[2]),\n",
        "        'training_samples': int(len(X_train)),\n",
        "        'test_samples': int(len(X_test)),\n",
        "        'symbols': symbols\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('/content/models/metadata.json', 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(\"Metadata saved\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_zip"
      },
      "outputs": [],
      "source": [
        "# Create deployment package\n",
        "def create_zip():\n",
        "    with zipfile.ZipFile('/content/trained_models.zip', 'w') as zipf:\n",
        "        for root, dirs, files in os.walk('/content/models'):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                arcname = os.path.relpath(file_path, '/content')\n",
        "                zipf.write(file_path, arcname)\n",
        "\n",
        "create_zip()\n",
        "print(\"Created trained_models.zip\")\n",
        "\n",
        "# Show what's in the package\n",
        "print(\"\\nPackage contents:\")\n",
        "with zipfile.ZipFile('/content/trained_models.zip', 'r') as zipf:\n",
        "    for name in zipf.namelist():\n",
        "        print(f\"  {name}\")\n",
        "\n",
        "print(f\"\\nPackage size: {os.path.getsize('/content/trained_models.zip') / 1024 / 1024:.1f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download"
      },
      "outputs": [],
      "source": [
        "# Download the trained models\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Downloading trained models...\")\n",
        "files.download('/content/trained_models.zip')\n",
        "\n",
        "print(\"\\nðŸŽ‰ Training Complete!\")\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"1. Extract trained_models.zip\")\n",
        "print(\"2. Upload models/ folder to your Vercel project\")\n",
        "print(\"3. Update predict-tft.js and predict-nhits.js to load these models:\")\n",
        "print(\"   await tf.loadLayersModel('./models/tft-trained/model.json')\")\n",
        "print(\"   await tf.loadLayersModel('./models/nhits-trained/model.json')\")\n",
        "print(\"4. Deploy updated Vercel project\")\n",
        "\n",
        "print(f\"\\nModel Performance:\")\n",
        "print(f\"TFT: {tft_dir_acc:.1%} direction accuracy, {tft_mae:.6f} MAE\")\n",
        "print(f\"N-HITS: {nhits_dir_acc:.1%} direction accuracy, {nhits_mae:.6f} MAE\")"
      ]
    }
  ]
}